{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.0029842479870079413\n",
      "average test loss: 0.0029375928424997255\n",
      "epoch: 115\n",
      "average training loss: 0.0029123882519985164\n",
      "average test loss: 0.002867468254407868\n",
      "epoch: 116\n",
      "average training loss: 0.0028426523933269303\n",
      "average test loss: 0.0027993987023364753\n",
      "epoch: 117\n",
      "average training loss: 0.0027749736902906615\n",
      "average test loss: 0.002733319292019587\n",
      "epoch: 118\n",
      "average training loss: 0.0027092843819820486\n",
      "average test loss: 0.0026691631028370466\n",
      "epoch: 119\n",
      "average training loss: 0.0026455229363979444\n",
      "average test loss: 0.0026068752267747186\n",
      "epoch: 120\n",
      "average training loss: 0.002583628218032091\n",
      "average test loss: 0.0025463960664637852\n",
      "epoch: 121\n",
      "average training loss: 0.002523540542460978\n",
      "average test loss: 0.002487665056833066\n",
      "epoch: 122\n",
      "average training loss: 0.0024652003293500883\n",
      "average test loss: 0.00243062694062246\n",
      "epoch: 123\n",
      "average training loss: 0.0024085527323597276\n",
      "average test loss: 0.0023752307934046257\n",
      "epoch: 124\n",
      "average training loss: 0.002353544667472734\n",
      "average test loss: 0.0023214217872009613\n",
      "epoch: 125\n",
      "average training loss: 0.0023001224675681443\n",
      "average test loss: 0.0022691513222525828\n",
      "epoch: 126\n",
      "average training loss: 0.0022482364965767106\n",
      "average test loss: 0.002218371919298079\n",
      "epoch: 127\n",
      "average training loss: 0.0021978401952176627\n",
      "average test loss: 0.0021690378998755477\n",
      "epoch: 128\n",
      "average training loss: 0.0021488835179137395\n",
      "average test loss: 0.0021210988925304264\n",
      "epoch: 129\n",
      "average training loss: 0.00210132066927412\n",
      "average test loss: 0.0020745151159644593\n",
      "epoch: 130\n",
      "average training loss: 0.0020551078305716683\n",
      "average test loss: 0.0020292407170927618\n",
      "epoch: 131\n",
      "average training loss: 0.0020102025506168824\n",
      "average test loss: 0.0019852368895953987\n",
      "epoch: 132\n",
      "average training loss: 0.001966564200462445\n",
      "average test loss: 0.0019424659403739497\n",
      "epoch: 133\n",
      "average training loss: 0.0019241530879619804\n",
      "average test loss: 0.0019008859580935678\n",
      "epoch: 134\n",
      "average training loss: 0.0018829289217726054\n",
      "average test loss: 0.0018604599426907953\n",
      "epoch: 135\n",
      "average training loss: 0.0018428555628120461\n",
      "average test loss: 0.001821152833144879\n",
      "epoch: 136\n",
      "average training loss: 0.001803895823404138\n",
      "average test loss: 0.0017829295902629383\n",
      "epoch: 137\n",
      "average training loss: 0.0017660154683707173\n",
      "average test loss: 0.0017457570720580406\n",
      "epoch: 138\n",
      "average training loss: 0.0017291812443598995\n",
      "average test loss: 0.001709602400296717\n",
      "epoch: 139\n",
      "average training loss: 0.001693360584527802\n",
      "average test loss: 0.001674435121458373\n",
      "epoch: 140\n",
      "average training loss: 0.0016585221644217038\n",
      "average test loss: 0.0016402244255004916\n",
      "epoch: 141\n",
      "average training loss: 0.0016246349487440003\n",
      "average test loss: 0.0016069398297986481\n",
      "epoch: 142\n",
      "average training loss: 0.0015916695061605423\n",
      "average test loss: 0.0015745532400615048\n",
      "epoch: 143\n",
      "average training loss: 0.0015595974203760682\n",
      "average test loss: 0.0015430386665684637\n",
      "epoch: 144\n",
      "average training loss: 0.0015283913616953855\n",
      "average test loss: 0.0015123678858799394\n",
      "epoch: 145\n",
      "average training loss: 0.0014980250450796174\n",
      "average test loss: 0.0014825164762442\n",
      "epoch: 146\n",
      "average training loss: 0.001468472896308145\n",
      "average test loss: 0.0014534597103192937\n",
      "epoch: 147\n",
      "average training loss: 0.0014397101067622905\n",
      "average test loss: 0.001425173399184132\n",
      "epoch: 148\n",
      "average training loss: 0.001411712827564984\n",
      "average test loss: 0.0013976335831102915\n",
      "epoch: 149\n",
      "average training loss: 0.001384457091883044\n",
      "average test loss: 0.0013708189489989309\n",
      "epoch: 150\n",
      "average training loss: 0.0013579205027781428\n",
      "average test loss: 0.001344705730843998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.rnn as rnn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "TIME_STEPS=10\n",
    "BATCH_SIZE=128\n",
    "HIDDEN_UNITS=1\n",
    "LEARNING_RATE=0.001\n",
    "EPOCH=150\n",
    "\n",
    "TRAIN_EXAMPLES=11000\n",
    "TEST_EXAMPLES=1100\n",
    "\n",
    "#------------------------------------Generate Data-----------------------------------------------#\n",
    "#generate data\n",
    "def generate(seq):\n",
    "    X=[]\n",
    "    y=[]\n",
    "    for i in range(len(seq)-TIME_STEPS):\n",
    "        X.append([seq[i:i+TIME_STEPS]])\n",
    "        y.append([seq[i+TIME_STEPS]])\n",
    "    return np.array(X,dtype=np.float32),np.array(y,dtype=np.float32)\n",
    "\n",
    "#s=[i for i in range(30)]\n",
    "#X,y=generate(s)\n",
    "#print(X)\n",
    "#print(y)\n",
    "\n",
    "seq_train=np.sin(np.linspace(start=0,stop=100,num=TRAIN_EXAMPLES,dtype=np.float32))\n",
    "seq_test=np.sin(np.linspace(start=100,stop=110,num=TEST_EXAMPLES,dtype=np.float32))\n",
    "\n",
    "#plt.plot(np.linspace(start=0,stop=100,num=10000,dtype=np.float32),seq_train)\n",
    "\n",
    "#plt.plot(np.linspace(start=100,stop=110,num=1000,dtype=np.float32),seq_test)\n",
    "#plt.show()\n",
    "\n",
    "X_train,y_train=generate(seq_train)\n",
    "#print(X_train.shape,y_train.shape)\n",
    "X_test,y_test=generate(seq_test)\n",
    "\n",
    "#reshape to (batch,time_steps,input_size)\n",
    "X_train=np.reshape(X_train,newshape=(-1,TIME_STEPS,1))\n",
    "X_test=np.reshape(X_test,newshape=(-1,TIME_STEPS,1))\n",
    "\n",
    "#draw y_test\n",
    "plt.plot(range(1000),y_test[:1000,0],\"r*\")\n",
    "#print(X_train.shape)\n",
    "#print(X_test.shape)\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "#--------------------------------------Define Graph---------------------------------------------------#\n",
    "graph=tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    #------------------------------------construct LSTM------------------------------------------#\n",
    "    #place hoder\n",
    "    X_p=tf.placeholder(dtype=tf.float32,shape=(None,TIME_STEPS,1),name=\"input_placeholder\")\n",
    "    y_p=tf.placeholder(dtype=tf.float32,shape=(None,1),name=\"pred_placeholder\")\n",
    "\n",
    "    #lstm instance\n",
    "    lstm_cell=rnn.BasicLSTMCell(num_units=HIDDEN_UNITS)\n",
    "\n",
    "    #initialize to zero\n",
    "    init_state=lstm_cell.zero_state(batch_size=BATCH_SIZE,dtype=tf.float32)\n",
    "\n",
    "    #dynamic rnn\n",
    "    outputs,states=tf.nn.dynamic_rnn(cell=lstm_cell,inputs=X_p,initial_state=init_state,dtype=tf.float32)\n",
    "    #print(outputs.shape)\n",
    "    h=outputs[:,-1,:]\n",
    "    #print(h.shape)\n",
    "    #--------------------------------------------------------------------------------------------#\n",
    "\n",
    "    #---------------------------------define loss and optimizer----------------------------------#\n",
    "    mse=tf.losses.mean_squared_error(labels=y_p,predictions=h)\n",
    "    #print(loss.shape)\n",
    "    optimizer=tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss=mse)\n",
    "\n",
    "\n",
    "    init=tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "#-------------------------------------------Define Session---------------------------------------#\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(1,EPOCH+1):\n",
    "        results = np.zeros(shape=(TEST_EXAMPLES, 1))\n",
    "        train_losses=[]\n",
    "        test_losses=[]\n",
    "        print(\"epoch:\",epoch)\n",
    "        for j in range(TRAIN_EXAMPLES//BATCH_SIZE):\n",
    "            _,train_loss=sess.run(\n",
    "                    fetches=(optimizer,mse),\n",
    "                    feed_dict={\n",
    "                            X_p:X_train[j*BATCH_SIZE:(j+1)*BATCH_SIZE],\n",
    "                            y_p:y_train[j*BATCH_SIZE:(j+1)*BATCH_SIZE]\n",
    "                        }\n",
    "            )\n",
    "            train_losses.append(train_loss)\n",
    "        print(\"average training loss:\", sum(train_losses) / len(train_losses))\n",
    "\n",
    "\n",
    "        for j in range(TEST_EXAMPLES//BATCH_SIZE):\n",
    "            result,test_loss=sess.run(\n",
    "                    fetches=(h,mse),\n",
    "                    feed_dict={\n",
    "                            X_p:X_test[j*BATCH_SIZE:(j+1)*BATCH_SIZE],\n",
    "                            y_p:y_test[j*BATCH_SIZE:(j+1)*BATCH_SIZE]\n",
    "                        }\n",
    "            )\n",
    "            results[j*BATCH_SIZE:(j+1)*BATCH_SIZE]=result\n",
    "            test_losses.append(test_loss)\n",
    "        print(\"average test loss:\", sum(test_losses) / len(test_losses))\n",
    "        plt.plot(range(1000),results[:1000,0])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
