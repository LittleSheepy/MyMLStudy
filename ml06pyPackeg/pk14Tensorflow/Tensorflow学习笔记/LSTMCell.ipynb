{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTMCell"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# LSTMCell\n",
    "***\n",
    "__init__(  \n",
    "\tnum_units,                 # int，LSTM单元中的单元数。  \n",
    "    use_peepholes=False,       # 要是为True 的话，表示使用diagonal/peephole连接。\n",
    "    cell_clip=None,            # \n",
    "    initializer=None,          # 权重和后面投射层（projection）的矩阵权重初始化方式。\n",
    "    num_proj=None,             # 可以简单理解为一个全连接，表示投射（projection）操作之后输出的维度，要是为None的话，表示不进行投射操作。\n",
    "    proj_clip=None,\n",
    "    num_unit_shards=None,\n",
    "    num_proj_shards=None,\n",
    "    forget_bias=1.0,\n",
    "    state_is_tuple=True,\n",
    "    activation=None,\n",
    "    reuse=None,\n",
    "    name=None,\n",
    "    dtype=None,\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_size: 64\n",
      "state_size: LSTMStateTuple(c=128, h=64)\n",
      "64\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers.python.layers import initializers\n",
    "\n",
    "lstm_cell=tf.nn.rnn_cell.LSTMCell(\n",
    "    num_units=128,\n",
    "    use_peepholes=True,\n",
    "    initializer=initializers.xavier_initializer(),\n",
    "    num_proj=64,\n",
    "    name=\"LSTM_CELL\"\n",
    ")\n",
    "\n",
    "print(\"output_size:\",lstm_cell.output_size)\n",
    "print(\"state_size:\",lstm_cell.state_size)\n",
    "print(lstm_cell.state_size.h)\n",
    "print(lstm_cell.state_size.c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BasicLSTMCell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MultiRNNCell"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "__init__( cells,state_is_tuple=True)\n",
    "参数:\n",
    "cells:一个列表,里面是你想叠起来的RNNCells,\n",
    "state_is_tuple:要是是True 的话, 以后都是默认是True了，因此这个参数不用管。接受和返回的state都是n-tuple,其中n = len(cells)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_size: 64\n",
      "state_size: <class 'tuple'>\n",
      "state_size: (LSTMStateTuple(c=128, h=64), LSTMStateTuple(c=128, h=64), LSTMStateTuple(c=128, h=64))\n",
      "64\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers.python.layers import initializers\n",
    "\n",
    "lstm_cell_1=tf.nn.rnn_cell.LSTMCell(\n",
    "    num_units=128,\n",
    "    use_peepholes=True,\n",
    "    initializer=initializers.xavier_initializer(),\n",
    "    num_proj=64,\n",
    "    name=\"LSTM_CELL_1\"\n",
    ")\n",
    "\n",
    "lstm_cell_2=tf.nn.rnn_cell.LSTMCell(\n",
    "    num_units=128,\n",
    "    use_peepholes=True,\n",
    "    initializer=initializers.xavier_initializer(),\n",
    "    num_proj=64,\n",
    "    name=\"LSTM_CELL_2\"\n",
    ")\n",
    "\n",
    "\n",
    "lstm_cell_3=tf.nn.rnn_cell.LSTMCell(\n",
    "    num_units=128,\n",
    "    use_peepholes=True,\n",
    "    initializer=initializers.xavier_initializer(),\n",
    "    num_proj=64,\n",
    "    name=\"LSTM_CELL_3\"\n",
    ")\n",
    "\n",
    "multi_cell=tf.nn.rnn_cell.MultiRNNCell(cells=[lstm_cell_1,lstm_cell_2,lstm_cell_3])\n",
    "\n",
    "\n",
    "print(\"output_size:\",multi_cell.output_size)\n",
    "print(\"state_size:\",type(multi_cell.state_size))\n",
    "print(\"state_size:\",multi_cell.state_size)\n",
    "\n",
    "#需要先索引到具体的那层cell，然后取出具体的state状态\n",
    "print(multi_cell.state_size[0].h)\n",
    "print(multi_cell.state_size[0].c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dynamic_rnn"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "dynamic_rnn(cell,inputs,sequence_length=None,initial_state=None,dtype=None,parallel_iterations=None,swap_memory=False,time_major=False,scope=None)\n",
    "cell: RNNCell的对象.\n",
    "inputs: RNN的输入,当time_major == False (default) 的时候,必须是形状为 [batch_size, max_time, ...] 的tensor, 要是 time_major == True 的话, 必须是形状为 [max_time, batch_size, ...] 的tensor. 前面两个维度应该在所有的输入里面都应该匹配.\n",
    "sequence_length: 可选,一个int32/int64类型的vector,他的尺寸是[batch_size]. 对于最后结果的正确性,这个还是非常有用的.因为给他具体每一个序列的长度,能够精确的得到结果,排除了之前为了把所有的序列弄成一样的长度padding造成的不准确.\n",
    "initial_state: 可选,RNN的初始状态. 要是cell.state_size 是一个整形,那么这个参数必须是一个形状为 [batch_size, cell.state_size] 的tensor. 要是cell.state_size 是一个tuple, 那么这个参数必须是一个tuple,其中元素为形状为[batch_size, s] 的tensor,s为cell.state_size 中的各个相应size.\n",
    "dtype: 可选,表示输入的数据类型和期望输出的数据类型.当初始状态没有被提供或者RNN的状态由多种形式构成的时候需要显示指定.\n",
    "parallel_iterations: 默认是32,表示的是并行运行的迭代数量(Default: 32). 有一些没有任何时间依赖的操作能够并行计算,实际上就是空间换时间和时间换空间的折中,当value远大于1的时候,会使用的更多的内存但是能够减少时间,当这个value值很小的时候,会使用小一点的内存,但是会花更多的时间.\n",
    "swap_memory: Transparently swap the tensors produced in forward inference but needed for back prop from GPU to CPU. This allows training RNNs which would typically not fit on a single GPU, with very minimal (or no) performance penalty.\n",
    "time_major: 规定了输入和输出tensor的数据组织格式,如果 true, tensor的形状需要是[max_time, batch_size, depth]. 若是false, 那么tensor的形状为[batch_size, max_time, depth]. 要是使用time_major = True 的话,会更加高效率一点,因为避免了在RNN计算的开始和结束的时候对于矩阵的转置 ,然而,大多数的tensorflow数据格式都是采用的以batch为主的格式,所以这里也默认采用以batch为主的格式.\n",
    "scope: 子图的scope名称,默认是\"rnn\"\n",
    "返回（非常重要）:\n",
    "返回(outputs, state)形式的结果对,其中：\n",
    "\n",
    "outputs: 表示RNN的输出隐状态h，就是所有时间步的h，要是time_major == False (default),那么这个tensor的形状为[batch_size, max_time, cell.output_size],要是time_major == True, 这个Tensor的形状为[max_time, batch_size, cell.output_size]. 这里需要注意一点，要是是双向LSTM，那么outputs就会是一个tuple，其中两个元素分别表示前向的outputs和反向的outputs，后面讲到双向LSTM会详细说这个内容。\n",
    "state: 最终时间步的states,要是单向网络，假如有K层，states就是一个元组，里面包含K（层数）个LSTMStateTuple，分别代表这些层最终的状态信息。要是是双向网络，那么还是元组，元组里面又是两个小元组分别表示前向的states和后向的states。相应的小元组里面就是每一层的最终时刻的states信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_size: 128\n",
      "state_size: LSTMStateTuple(c=128, h=128)\n",
      "output.shape: (32, 40, 128)\n",
      "len of state tuple 2\n",
      "state.h.shape: (32, 128)\n",
      "state.c.shape: (32, 128)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "inputs = tf.placeholder(np.float32, shape=(32,40,5)) # 32 是 batch_size\n",
    "lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(num_units=128)\n",
    "#lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(num_units=256)\n",
    "#lstm_cell_3 = tf.contrib.rnn.BasicLSTMCell(num_units=512)\n",
    "#多层lstm_cell\n",
    "#lstm_cell=tf.contrib.rnn.MultiRNNCell(cells=[lstm_cell_1,lstm_cell_2,lstm_cell_3])\n",
    "\n",
    "print(\"output_size:\",lstm_cell_1.output_size)\n",
    "print(\"state_size:\",lstm_cell_1.state_size)\n",
    "#print(lstm_cell.state_size.h)\n",
    "#print(lstm_cell.state_size.c)\n",
    "output,state=tf.nn.dynamic_rnn(\n",
    "    cell=lstm_cell_1,\n",
    "    inputs=inputs,\n",
    "    dtype=tf.float32\n",
    ")\n",
    "\n",
    "print(\"output.shape:\",output.shape)\n",
    "print(\"len of state tuple\",len(state))\n",
    "print(\"state.h.shape:\",state.h.shape)\n",
    "print(\"state.c.shape:\",state.c.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_size: 512\n",
      "state_size: (LSTMStateTuple(c=128, h=128), LSTMStateTuple(c=256, h=256), LSTMStateTuple(c=512, h=512))\n",
      "output.shape: (32, 40, 512)\n",
      "len of state tuple 3\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "inputs = tf.placeholder(np.float32, shape=(32,40,5)) # 32 是 batch_size\n",
    "lstm_cell_1 = tf.contrib.rnn.BasicLSTMCell(num_units=128)\n",
    "lstm_cell_2 = tf.contrib.rnn.BasicLSTMCell(num_units=256)\n",
    "lstm_cell_3 = tf.contrib.rnn.BasicLSTMCell(num_units=512)\n",
    "#多层lstm_cell\n",
    "lstm_cell=tf.contrib.rnn.MultiRNNCell(cells=[lstm_cell_1,lstm_cell_2,lstm_cell_3])\n",
    "\n",
    "print(\"output_size:\",lstm_cell.output_size)\n",
    "print(\"state_size:\",lstm_cell.state_size)\n",
    "#print(lstm_cell.state_size.h)\n",
    "#print(lstm_cell.state_size.c)\n",
    "output,state=tf.nn.dynamic_rnn(\n",
    "    cell=lstm_cell,\n",
    "    inputs=inputs,\n",
    "    dtype=tf.float32\n",
    ")\n",
    "\n",
    "print(\"output.shape:\",output.shape)\n",
    "print(\"len of state tuple\",len(state))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bidirectional_dynamic_rnn"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bidirectional_dynamic_rnn(cell_fw,cell_bw,inputs,sequence_length=None,initial_state_fw=None,initial_state_bw=None,dtype=None,parallel_iterations=None,swap_memory=False,time_major=False,scope=None)\n",
    "cell_fw：RNNCell的一个实例，用于正向。\n",
    "cell_bw：RNNCell的一个实例，用于反向。\n",
    "inputs：RNN输入。如果time_major == False（默认），则它必须是形状为 [batch_size, max_time, ...]的tensor，或者这些元素的嵌套元组。如果time_major == True，则它必须是形状为[max_time, batch_size, ...]的tensor ，或者是这些元素的嵌套元组。\n",
    "sequence_length：（可选）一个int32 / int64向量，大小[batch_size]，包含批处理中每个序列的实际长度。如果未提供，则所有批次条目均假定为完整序列; 并且时间反转从时间0到max_time每个序列被应用。\n",
    "initial_state_fw:(可选）前向RNN的初始状态。这必须是适当类型和形状的张量[batch_size, cell_fw.state_size]。如果cell_fw.state_size是一个元组，这应该是一个具有形状的张量的元组[batch_size, s] for s in cell_fw.state_size。\n",
    "initial_state_bw：（可选）与之相同initial_state_fw，但使用相应的属性cell_bw。\n",
    "dtype:(可选）初始状态和预期输出的数据类型。如果未提供initial_states或者RNN状态具有异构dtype，则为必需。\n",
    "parallel_iterations:(默认：32）。并行运行的迭代次数。那些没有任何时间依赖性并且可以并行运行的操作将会是。此参数用于空间换算时间。值>> 1使用更多的内存，但花费更少的时间，而更小的值使用更少的内存，但计算需要更长的时间。\n",
    "swap_memory：透明地交换前向推理中产生的张量，但是从GPU到后端支持所需的张量。这允许训练通常不适合单个GPU的RNN，而且性能损失非常小（或不）。\n",
    "time_major：inputs和outputs张量的形状格式。如果为True的话，这些都Tensors的形状为[max_time, batch_size, depth]。如果为False的话，这些Tensors的形状是[batch_size, max_time, depth]。\n",
    "scope：创建子图的VariableScope; 默认为“bidirectional_rnn”\n",
    "元组（outputs，output_states） 其中\n",
    "outputs:包含正向和反向rnn输出的元组（output_fw，output_bw）。\n",
    "如果time_major == False（默认值），则output_fw将是一个形状为[batch_size, max_time, cell_fw.output_size] 的tensor,output_bw将是一个形状为[batch_size, max_time, cell_bw.output_size]的tensor.\n",
    "如果time_major == True，则output_fw将为一个形状为[max_time, batch_size, cell_fw.output_size] 的tensor, output_bw将是一个形状为[max_time, batch_size, cell_bw.output_size] 的tensor.\n",
    "output_state,也是一个tuple,内容是(output_state_fw, output_state_bw) 也就是说,前向的state和后向的state放到了一个元组里面."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_fw_size: 128\n",
      "state_fw_size: LSTMStateTuple(c=128, h=128)\n",
      "output_bw_size: 128\n",
      "state_bw_size: LSTMStateTuple(c=128, h=128)\n",
      "output_fw.shape: (32, 40, 128)\n",
      "output_bw.shape: (32, 40, 128)\n",
      "len of state tuple 2\n",
      "state_fw: LSTMStateTuple(c=<tf.Tensor 'bidirectional_rnn/fw/fw/while/Exit_3:0' shape=(32, 128) dtype=float32>, h=<tf.Tensor 'bidirectional_rnn/fw/fw/while/Exit_4:0' shape=(32, 128) dtype=float32>)\n",
      "state_bw: LSTMStateTuple(c=<tf.Tensor 'bidirectional_rnn/bw/bw/while/Exit_3:0' shape=(32, 128) dtype=float32>, h=<tf.Tensor 'bidirectional_rnn/bw/bw/while/Exit_4:0' shape=(32, 128) dtype=float32>)\n",
      "state_fw_h_concat.shape (32, 256)\n",
      "state_fw_h_concat.shape (32, 256)\n",
      "LSTMStateTuple(c=<tf.Tensor 'concat_1:0' shape=(32, 256) dtype=float32>, h=<tf.Tensor 'concat:0' shape=(32, 256) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "inputs = tf.placeholder(np.float32, shape=(32,40,5)) # 32 是 batch_size\n",
    "lstm_cell_fw = tf.contrib.rnn.BasicLSTMCell(num_units=128)\n",
    "lstm_cell_bw = tf.contrib.rnn.BasicLSTMCell(num_units=128)\n",
    "\n",
    "#多层lstm_cell\n",
    "#lstm_cell=tf.contrib.rnn.MultiRNNCell(cells=[lstm_cell_1,lstm_cell_2,lstm_cell_3])\n",
    "\n",
    "print(\"output_fw_size:\",lstm_cell_fw.output_size)\n",
    "print(\"state_fw_size:\",lstm_cell_fw.state_size)\n",
    "print(\"output_bw_size:\",lstm_cell_bw.output_size)\n",
    "print(\"state_bw_size:\",lstm_cell_bw.state_size)\n",
    "\n",
    "#print(lstm_cell.state_size.h)\n",
    "#print(lstm_cell.state_size.c)\n",
    "output,state=tf.nn.bidirectional_dynamic_rnn(\n",
    "    cell_fw=lstm_cell_fw,\n",
    "    cell_bw=lstm_cell_bw,\n",
    "    inputs=inputs,\n",
    "    dtype=tf.float32\n",
    ")\n",
    "output_fw=output[0]\n",
    "output_bw=output[1]\n",
    "state_fw=state[0]\n",
    "state_bw=state[1]\n",
    "\n",
    "print(\"output_fw.shape:\",output_fw.shape)\n",
    "print(\"output_bw.shape:\",output_bw.shape)\n",
    "print(\"len of state tuple\",len(state_fw))\n",
    "print(\"state_fw:\",state_fw)\n",
    "print(\"state_bw:\",state_bw)\n",
    "#print(\"state.h.shape:\",state.h.shape)\n",
    "#print(\"state.c.shape:\",state.c.shape)\n",
    "\n",
    "#state_concat=tf.concat(values=[state_fw,state_fw],axis=1)\n",
    "#print(state_concat)\n",
    "state_h_concat=tf.concat(values=[state_fw.h,state_bw.h],axis=1)\n",
    "print(\"state_fw_h_concat.shape\",state_h_concat.shape)\n",
    "\n",
    "state_c_concat=tf.concat(values=[state_fw.c,state_bw.c],axis=1)\n",
    "print(\"state_fw_h_concat.shape\",state_c_concat.shape)\n",
    "\n",
    "state_concat=tf.contrib.rnn.LSTMStateTuple(c=state_c_concat,h=state_h_concat)\n",
    "print(state_concat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
