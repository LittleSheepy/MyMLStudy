{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据获取及预处理\n",
    "class MNISTLoader():\n",
    "    def __init__(self):\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()\n",
    "        # MNIST中的图像默认为uint8（0-255的数字）。以下代码将其归一化到0-1之间的浮点数，并在最后增加一维作为颜色通道\n",
    "        print(self.train_data.shape)\n",
    "        self.train_data = np.expand_dims(self.train_data.astype(np.float32) / 255.0, axis=-1)      # [60000, 28, 28, 1]\n",
    "        print(self.train_data.shape)\n",
    "        self.test_data = np.expand_dims(self.test_data.astype(np.float32) / 255.0, axis=-1)        # [10000, 28, 28, 1]\n",
    "        self.train_label = self.train_label.astype(np.int32)    # [60000]\n",
    "        self.test_label = self.test_label.astype(np.int32)      # [10000]\n",
    "        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        # 从数据集中随机取出batch_size个元素并返回\n",
    "        index = np.random.randint(0, np.shape(self.train_data)[0], batch_size)\n",
    "        return self.train_data[index, :], self.train_label[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(100, activation=tf.nn.relu),\n",
    "    tf.keras.layers.Dense(10),\n",
    "    tf.keras.layers.Softmax()\n",
    "])\n",
    "data_loader = MNISTLoader()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: loss 2.388603\n",
      "batch 1: loss 2.293674\n",
      "batch 2: loss 2.186231\n",
      "batch 3: loss 2.086780\n",
      "batch 4: loss 2.045081\n",
      "batch 5: loss 1.998972\n",
      "batch 6: loss 1.872199\n",
      "batch 7: loss 1.832567\n",
      "batch 8: loss 1.783183\n",
      "batch 9: loss 1.751624\n",
      "batch 10: loss 1.685742\n",
      "batch 11: loss 1.533611\n",
      "batch 12: loss 1.443546\n",
      "batch 13: loss 1.487695\n",
      "batch 14: loss 1.356239\n",
      "batch 15: loss 1.409653\n",
      "batch 16: loss 1.215031\n",
      "batch 17: loss 1.214518\n",
      "batch 18: loss 1.089001\n",
      "batch 19: loss 1.206711\n",
      "batch 20: loss 1.163121\n",
      "batch 21: loss 0.981417\n",
      "batch 22: loss 1.039429\n",
      "batch 23: loss 1.138160\n",
      "batch 24: loss 0.872527\n",
      "batch 25: loss 0.863287\n",
      "batch 26: loss 0.899236\n",
      "batch 27: loss 0.783926\n",
      "batch 28: loss 0.875778\n",
      "batch 29: loss 0.787400\n",
      "batch 30: loss 0.737174\n",
      "batch 31: loss 0.700911\n",
      "batch 32: loss 0.807517\n",
      "batch 33: loss 0.714060\n",
      "batch 34: loss 0.688951\n",
      "batch 35: loss 0.769232\n",
      "batch 36: loss 0.544162\n",
      "batch 37: loss 0.709931\n",
      "batch 38: loss 0.692744\n",
      "batch 39: loss 0.493657\n",
      "batch 40: loss 0.650259\n",
      "batch 41: loss 0.660416\n",
      "batch 42: loss 0.687391\n",
      "batch 43: loss 0.655100\n",
      "batch 44: loss 0.660177\n",
      "batch 45: loss 0.808994\n",
      "batch 46: loss 0.556366\n",
      "batch 47: loss 0.477597\n",
      "batch 48: loss 0.501252\n",
      "batch 49: loss 0.526118\n",
      "batch 50: loss 0.584702\n",
      "batch 51: loss 0.392132\n",
      "batch 52: loss 0.611221\n",
      "batch 53: loss 0.525846\n",
      "batch 54: loss 0.547054\n",
      "batch 55: loss 0.465985\n",
      "batch 56: loss 0.616862\n",
      "batch 57: loss 0.514932\n",
      "batch 58: loss 0.480934\n",
      "batch 59: loss 0.501361\n",
      "batch 60: loss 0.474610\n",
      "batch 61: loss 0.492557\n",
      "batch 62: loss 0.530750\n",
      "batch 63: loss 0.442696\n",
      "batch 64: loss 0.455954\n",
      "batch 65: loss 0.486198\n",
      "batch 66: loss 0.369750\n",
      "batch 67: loss 0.427564\n",
      "batch 68: loss 0.433361\n",
      "batch 69: loss 0.496665\n",
      "batch 70: loss 0.327801\n",
      "batch 71: loss 0.332193\n",
      "batch 72: loss 0.557196\n",
      "batch 73: loss 0.446863\n",
      "batch 74: loss 0.429107\n",
      "batch 75: loss 0.482550\n",
      "batch 76: loss 0.403358\n",
      "batch 77: loss 0.345061\n",
      "batch 78: loss 0.349616\n",
      "batch 79: loss 0.342467\n",
      "batch 80: loss 0.483805\n",
      "batch 81: loss 0.472413\n",
      "batch 82: loss 0.502089\n",
      "batch 83: loss 0.414589\n",
      "batch 84: loss 0.312943\n",
      "batch 85: loss 0.341691\n",
      "batch 86: loss 0.438971\n",
      "batch 87: loss 0.254298\n",
      "batch 88: loss 0.296420\n",
      "batch 89: loss 0.282465\n",
      "batch 90: loss 0.449741\n",
      "batch 91: loss 0.303563\n",
      "batch 92: loss 0.353118\n",
      "batch 93: loss 0.351434\n",
      "batch 94: loss 0.435785\n",
      "batch 95: loss 0.638394\n",
      "batch 96: loss 0.238845\n",
      "batch 97: loss 0.402965\n",
      "batch 98: loss 0.491437\n",
      "batch 99: loss 0.406880\n",
      "batch 100: loss 0.418643\n",
      "batch 101: loss 0.402746\n",
      "batch 102: loss 0.339260\n",
      "batch 103: loss 0.351733\n",
      "batch 104: loss 0.389651\n",
      "batch 105: loss 0.402644\n",
      "batch 106: loss 0.377590\n",
      "batch 107: loss 0.461020\n",
      "batch 108: loss 0.470320\n",
      "batch 109: loss 0.382204\n",
      "batch 110: loss 0.299731\n",
      "batch 111: loss 0.366202\n",
      "batch 112: loss 0.432336\n",
      "batch 113: loss 0.264384\n",
      "batch 114: loss 0.314323\n",
      "batch 115: loss 0.444611\n",
      "batch 116: loss 0.321588\n",
      "batch 117: loss 0.298574\n",
      "batch 118: loss 0.363961\n",
      "batch 119: loss 0.374260\n",
      "batch 120: loss 0.279912\n",
      "batch 121: loss 0.422739\n",
      "batch 122: loss 0.300737\n",
      "batch 123: loss 0.409896\n",
      "batch 124: loss 0.391535\n",
      "batch 125: loss 0.347066\n",
      "batch 126: loss 0.505815\n",
      "batch 127: loss 0.235813\n",
      "batch 128: loss 0.261774\n",
      "batch 129: loss 0.359224\n",
      "batch 130: loss 0.367258\n",
      "batch 131: loss 0.504869\n",
      "batch 132: loss 0.457129\n",
      "batch 133: loss 0.386065\n",
      "batch 134: loss 0.399697\n",
      "batch 135: loss 0.315465\n",
      "batch 136: loss 0.339609\n",
      "batch 137: loss 0.481716\n",
      "batch 138: loss 0.380424\n",
      "batch 139: loss 0.280190\n",
      "batch 140: loss 0.442493\n",
      "batch 141: loss 0.318199\n",
      "batch 142: loss 0.386014\n",
      "batch 143: loss 0.325858\n",
      "batch 144: loss 0.315029\n",
      "batch 145: loss 0.343774\n",
      "batch 146: loss 0.495753\n",
      "batch 147: loss 0.275958\n",
      "batch 148: loss 0.218363\n",
      "batch 149: loss 0.299920\n",
      "batch 150: loss 0.353316\n",
      "batch 151: loss 0.385024\n",
      "batch 152: loss 0.368284\n",
      "batch 153: loss 0.314662\n",
      "batch 154: loss 0.284045\n",
      "batch 155: loss 0.264050\n",
      "batch 156: loss 0.318248\n",
      "batch 157: loss 0.336366\n",
      "batch 158: loss 0.343398\n",
      "batch 159: loss 0.362479\n",
      "batch 160: loss 0.334275\n",
      "batch 161: loss 0.349313\n",
      "batch 162: loss 0.300083\n",
      "batch 163: loss 0.371487\n",
      "batch 164: loss 0.323746\n",
      "batch 165: loss 0.361543\n",
      "batch 166: loss 0.368176\n",
      "batch 167: loss 0.279659\n",
      "batch 168: loss 0.284840\n",
      "batch 169: loss 0.288039\n",
      "batch 170: loss 0.369138\n",
      "batch 171: loss 0.264452\n",
      "batch 172: loss 0.370088\n",
      "batch 173: loss 0.354241\n",
      "batch 174: loss 0.287736\n",
      "batch 175: loss 0.493029\n",
      "batch 176: loss 0.303657\n",
      "batch 177: loss 0.298107\n",
      "batch 178: loss 0.175920\n",
      "batch 179: loss 0.246352\n",
      "batch 180: loss 0.388392\n",
      "batch 181: loss 0.244323\n",
      "batch 182: loss 0.383027\n",
      "batch 183: loss 0.400122\n",
      "batch 184: loss 0.220737\n",
      "batch 185: loss 0.227751\n",
      "batch 186: loss 0.266851\n",
      "batch 187: loss 0.276582\n",
      "batch 188: loss 0.551835\n",
      "batch 189: loss 0.407404\n",
      "batch 190: loss 0.223379\n",
      "batch 191: loss 0.241463\n",
      "batch 192: loss 0.302960\n",
      "batch 193: loss 0.231976\n",
      "batch 194: loss 0.164207\n",
      "batch 195: loss 0.280163\n",
      "batch 196: loss 0.321815\n",
      "batch 197: loss 0.332176\n",
      "batch 198: loss 0.206803\n",
      "batch 199: loss 0.333911\n",
      "batch 200: loss 0.278646\n",
      "batch 201: loss 0.138244\n",
      "batch 202: loss 0.327864\n",
      "batch 203: loss 0.345106\n",
      "batch 204: loss 0.226138\n",
      "batch 205: loss 0.290675\n",
      "batch 206: loss 0.293313\n",
      "batch 207: loss 0.315824\n",
      "batch 208: loss 0.204928\n",
      "batch 209: loss 0.161271\n",
      "batch 210: loss 0.252029\n",
      "batch 211: loss 0.273123\n",
      "batch 212: loss 0.370349\n",
      "batch 213: loss 0.254228\n",
      "batch 214: loss 0.318664\n",
      "batch 215: loss 0.209229\n",
      "batch 216: loss 0.273038\n",
      "batch 217: loss 0.283659\n",
      "batch 218: loss 0.225487\n",
      "batch 219: loss 0.212430\n",
      "batch 220: loss 0.270366\n",
      "batch 221: loss 0.283784\n",
      "batch 222: loss 0.209505\n",
      "batch 223: loss 0.278342\n",
      "batch 224: loss 0.435201\n",
      "batch 225: loss 0.175742\n",
      "batch 226: loss 0.214805\n",
      "batch 227: loss 0.340673\n",
      "batch 228: loss 0.296892\n",
      "batch 229: loss 0.343698\n",
      "batch 230: loss 0.318072\n",
      "batch 231: loss 0.281056\n",
      "batch 232: loss 0.241397\n",
      "batch 233: loss 0.399454\n",
      "batch 234: loss 0.296539\n",
      "batch 235: loss 0.285407\n",
      "batch 236: loss 0.303220\n",
      "batch 237: loss 0.348957\n",
      "batch 238: loss 0.253968\n",
      "batch 239: loss 0.176234\n",
      "batch 240: loss 0.295110\n",
      "batch 241: loss 0.294611\n",
      "batch 242: loss 0.316611\n",
      "batch 243: loss 0.157905\n",
      "batch 244: loss 0.193894\n",
      "batch 245: loss 0.417901\n",
      "batch 246: loss 0.197378\n",
      "batch 247: loss 0.250636\n",
      "batch 248: loss 0.253446\n",
      "batch 249: loss 0.386329\n",
      "batch 250: loss 0.348268\n",
      "batch 251: loss 0.307658\n",
      "batch 252: loss 0.415888\n",
      "batch 253: loss 0.245012\n",
      "batch 254: loss 0.270494\n",
      "batch 255: loss 0.226534\n",
      "batch 256: loss 0.313812\n",
      "batch 257: loss 0.247466\n",
      "batch 258: loss 0.266780\n",
      "batch 259: loss 0.214050\n",
      "batch 260: loss 0.250063\n",
      "batch 261: loss 0.315287\n",
      "batch 262: loss 0.484304\n",
      "batch 263: loss 0.301180\n",
      "batch 264: loss 0.242736\n",
      "batch 265: loss 0.262376\n",
      "batch 266: loss 0.207468\n",
      "batch 267: loss 0.233220\n",
      "batch 268: loss 0.253907\n",
      "batch 269: loss 0.297192\n",
      "batch 270: loss 0.279439\n",
      "batch 271: loss 0.231205\n",
      "batch 272: loss 0.378479\n",
      "batch 273: loss 0.180630\n",
      "batch 274: loss 0.196533\n",
      "batch 275: loss 0.336125\n",
      "batch 276: loss 0.296079\n",
      "batch 277: loss 0.257163\n",
      "batch 278: loss 0.306977\n",
      "batch 279: loss 0.331763\n",
      "batch 280: loss 0.284710\n",
      "batch 281: loss 0.349779\n",
      "batch 282: loss 0.227706\n",
      "batch 283: loss 0.248789\n",
      "batch 284: loss 0.271434\n",
      "batch 285: loss 0.215374\n",
      "batch 286: loss 0.314665\n",
      "batch 287: loss 0.324572\n",
      "batch 288: loss 0.232781\n",
      "batch 289: loss 0.324651\n",
      "batch 290: loss 0.304873\n",
      "batch 291: loss 0.164983\n",
      "batch 292: loss 0.308344\n",
      "batch 293: loss 0.280721\n",
      "batch 294: loss 0.151491\n",
      "batch 295: loss 0.127736\n",
      "batch 296: loss 0.172643\n",
      "batch 297: loss 0.250590\n",
      "batch 298: loss 0.247703\n",
      "batch 299: loss 0.211425\n",
      "batch 300: loss 0.459011\n",
      "batch 301: loss 0.284215\n",
      "batch 302: loss 0.255245\n",
      "batch 303: loss 0.230218\n",
      "batch 304: loss 0.251357\n",
      "batch 305: loss 0.270175\n",
      "batch 306: loss 0.192353\n",
      "batch 307: loss 0.182151\n",
      "batch 308: loss 0.267543\n",
      "batch 309: loss 0.246334\n",
      "batch 310: loss 0.249697\n",
      "batch 311: loss 0.144323\n",
      "batch 312: loss 0.305671\n",
      "batch 313: loss 0.236042\n",
      "batch 314: loss 0.352875\n",
      "batch 315: loss 0.256727\n",
      "batch 316: loss 0.264274\n",
      "batch 317: loss 0.279558\n",
      "batch 318: loss 0.200849\n",
      "batch 319: loss 0.275990\n",
      "batch 320: loss 0.359267\n",
      "batch 321: loss 0.175000\n",
      "batch 322: loss 0.322870\n",
      "batch 323: loss 0.312464\n",
      "batch 324: loss 0.322836\n",
      "batch 325: loss 0.209105\n",
      "batch 326: loss 0.281012\n",
      "batch 327: loss 0.176144\n",
      "batch 328: loss 0.188033\n",
      "batch 329: loss 0.241478\n",
      "batch 330: loss 0.306585\n",
      "batch 331: loss 0.182476\n",
      "batch 332: loss 0.341744\n",
      "batch 333: loss 0.181523\n",
      "batch 334: loss 0.398401\n",
      "batch 335: loss 0.183990\n",
      "batch 336: loss 0.219768\n",
      "batch 337: loss 0.269640\n",
      "batch 338: loss 0.289088\n",
      "batch 339: loss 0.235591\n",
      "batch 340: loss 0.218885\n",
      "batch 341: loss 0.165628\n",
      "batch 342: loss 0.198838\n",
      "batch 343: loss 0.185456\n",
      "batch 344: loss 0.236372\n",
      "batch 345: loss 0.302997\n",
      "batch 346: loss 0.264484\n",
      "batch 347: loss 0.249762\n",
      "batch 348: loss 0.124020\n",
      "batch 349: loss 0.223776\n",
      "batch 350: loss 0.213377\n",
      "batch 351: loss 0.252063\n",
      "batch 352: loss 0.183102\n",
      "batch 353: loss 0.233215\n",
      "batch 354: loss 0.238791\n",
      "batch 355: loss 0.211687\n",
      "batch 356: loss 0.205751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 357: loss 0.265880\n",
      "batch 358: loss 0.228265\n",
      "batch 359: loss 0.348219\n",
      "batch 360: loss 0.176772\n",
      "batch 361: loss 0.242972\n",
      "batch 362: loss 0.268927\n",
      "batch 363: loss 0.217994\n",
      "batch 364: loss 0.376223\n",
      "batch 365: loss 0.203373\n",
      "batch 366: loss 0.271851\n",
      "batch 367: loss 0.169179\n",
      "batch 368: loss 0.436952\n",
      "batch 369: loss 0.354993\n",
      "batch 370: loss 0.495745\n",
      "batch 371: loss 0.203798\n",
      "batch 372: loss 0.171820\n",
      "batch 373: loss 0.192341\n",
      "batch 374: loss 0.369319\n",
      "batch 375: loss 0.151433\n",
      "batch 376: loss 0.113618\n",
      "batch 377: loss 0.187428\n",
      "batch 378: loss 0.260494\n",
      "batch 379: loss 0.216251\n",
      "batch 380: loss 0.143532\n",
      "batch 381: loss 0.290597\n",
      "batch 382: loss 0.182007\n",
      "batch 383: loss 0.230474\n",
      "batch 384: loss 0.438971\n",
      "batch 385: loss 0.264026\n",
      "batch 386: loss 0.166633\n",
      "batch 387: loss 0.204959\n",
      "batch 388: loss 0.215557\n",
      "batch 389: loss 0.168520\n",
      "batch 390: loss 0.173369\n",
      "batch 391: loss 0.201375\n",
      "batch 392: loss 0.167552\n",
      "batch 393: loss 0.175329\n",
      "batch 394: loss 0.209475\n",
      "batch 395: loss 0.333485\n",
      "batch 396: loss 0.182565\n",
      "batch 397: loss 0.227365\n",
      "batch 398: loss 0.192144\n",
      "batch 399: loss 0.147650\n",
      "batch 400: loss 0.287524\n",
      "batch 401: loss 0.324307\n",
      "batch 402: loss 0.181277\n",
      "batch 403: loss 0.230604\n",
      "batch 404: loss 0.289162\n",
      "batch 405: loss 0.190513\n",
      "batch 406: loss 0.179626\n",
      "batch 407: loss 0.181763\n",
      "batch 408: loss 0.262106\n",
      "batch 409: loss 0.142605\n",
      "batch 410: loss 0.223139\n",
      "batch 411: loss 0.237868\n",
      "batch 412: loss 0.222857\n",
      "batch 413: loss 0.183299\n",
      "batch 414: loss 0.307179\n",
      "batch 415: loss 0.257157\n",
      "batch 416: loss 0.250777\n",
      "batch 417: loss 0.194543\n",
      "batch 418: loss 0.235810\n",
      "batch 419: loss 0.202991\n",
      "batch 420: loss 0.236530\n",
      "batch 421: loss 0.203677\n",
      "batch 422: loss 0.213417\n",
      "batch 423: loss 0.211455\n",
      "batch 424: loss 0.127291\n",
      "batch 425: loss 0.242257\n",
      "batch 426: loss 0.260040\n",
      "batch 427: loss 0.193933\n",
      "batch 428: loss 0.302329\n",
      "batch 429: loss 0.332859\n",
      "batch 430: loss 0.204779\n",
      "batch 431: loss 0.330531\n",
      "batch 432: loss 0.206987\n",
      "batch 433: loss 0.303484\n",
      "batch 434: loss 0.135961\n",
      "batch 435: loss 0.194072\n",
      "batch 436: loss 0.255706\n",
      "batch 437: loss 0.347400\n",
      "batch 438: loss 0.171194\n",
      "batch 439: loss 0.280562\n",
      "batch 440: loss 0.360855\n",
      "batch 441: loss 0.182601\n",
      "batch 442: loss 0.148640\n",
      "batch 443: loss 0.267642\n",
      "batch 444: loss 0.111349\n",
      "batch 445: loss 0.210213\n",
      "batch 446: loss 0.105021\n",
      "batch 447: loss 0.248806\n",
      "batch 448: loss 0.204011\n",
      "batch 449: loss 0.181282\n",
      "batch 450: loss 0.146931\n",
      "batch 451: loss 0.256979\n",
      "batch 452: loss 0.310700\n",
      "batch 453: loss 0.094978\n",
      "batch 454: loss 0.177444\n",
      "batch 455: loss 0.179584\n",
      "batch 456: loss 0.233280\n",
      "batch 457: loss 0.349627\n",
      "batch 458: loss 0.296925\n",
      "batch 459: loss 0.309478\n",
      "batch 460: loss 0.135361\n",
      "batch 461: loss 0.237890\n",
      "batch 462: loss 0.318544\n",
      "batch 463: loss 0.193462\n",
      "batch 464: loss 0.243842\n",
      "batch 465: loss 0.247340\n",
      "batch 466: loss 0.222608\n",
      "batch 467: loss 0.278274\n",
      "batch 468: loss 0.213936\n",
      "batch 469: loss 0.192534\n",
      "batch 470: loss 0.222702\n",
      "batch 471: loss 0.295714\n",
      "batch 472: loss 0.167001\n",
      "batch 473: loss 0.212088\n",
      "batch 474: loss 0.161883\n",
      "batch 475: loss 0.190363\n",
      "batch 476: loss 0.314337\n",
      "batch 477: loss 0.218730\n",
      "batch 478: loss 0.307819\n",
      "batch 479: loss 0.194290\n",
      "batch 480: loss 0.208708\n",
      "batch 481: loss 0.232624\n",
      "batch 482: loss 0.249405\n",
      "batch 483: loss 0.212485\n",
      "batch 484: loss 0.292605\n",
      "batch 485: loss 0.269643\n",
      "batch 486: loss 0.134550\n",
      "batch 487: loss 0.224827\n",
      "batch 488: loss 0.160177\n",
      "batch 489: loss 0.159033\n",
      "batch 490: loss 0.154574\n",
      "batch 491: loss 0.188295\n",
      "batch 492: loss 0.250187\n",
      "batch 493: loss 0.186276\n",
      "batch 494: loss 0.288240\n",
      "batch 495: loss 0.204727\n",
      "batch 496: loss 0.215272\n",
      "batch 497: loss 0.220784\n",
      "batch 498: loss 0.229465\n",
      "batch 499: loss 0.246777\n",
      "batch 500: loss 0.255593\n",
      "batch 501: loss 0.243364\n",
      "batch 502: loss 0.144196\n",
      "batch 503: loss 0.220168\n",
      "batch 504: loss 0.265598\n",
      "batch 505: loss 0.176214\n",
      "batch 506: loss 0.203434\n",
      "batch 507: loss 0.257437\n",
      "batch 508: loss 0.215529\n",
      "batch 509: loss 0.183575\n",
      "batch 510: loss 0.162085\n",
      "batch 511: loss 0.295095\n",
      "batch 512: loss 0.196992\n",
      "batch 513: loss 0.276743\n",
      "batch 514: loss 0.123998\n",
      "batch 515: loss 0.183611\n",
      "batch 516: loss 0.266137\n",
      "batch 517: loss 0.235126\n",
      "batch 518: loss 0.428531\n",
      "batch 519: loss 0.149235\n",
      "batch 520: loss 0.245188\n",
      "batch 521: loss 0.316575\n",
      "batch 522: loss 0.148970\n",
      "batch 523: loss 0.168212\n",
      "batch 524: loss 0.115387\n",
      "batch 525: loss 0.175016\n",
      "batch 526: loss 0.175448\n",
      "batch 527: loss 0.175957\n",
      "batch 528: loss 0.221312\n",
      "batch 529: loss 0.186584\n",
      "batch 530: loss 0.235854\n",
      "batch 531: loss 0.207181\n",
      "batch 532: loss 0.182507\n",
      "batch 533: loss 0.160755\n",
      "batch 534: loss 0.195834\n",
      "batch 535: loss 0.210640\n",
      "batch 536: loss 0.226012\n",
      "batch 537: loss 0.206133\n",
      "batch 538: loss 0.225093\n",
      "batch 539: loss 0.317532\n",
      "batch 540: loss 0.384977\n",
      "batch 541: loss 0.268724\n",
      "batch 542: loss 0.166168\n",
      "batch 543: loss 0.093757\n",
      "batch 544: loss 0.230846\n",
      "batch 545: loss 0.423147\n",
      "batch 546: loss 0.339338\n",
      "batch 547: loss 0.159605\n",
      "batch 548: loss 0.175416\n",
      "batch 549: loss 0.162835\n",
      "batch 550: loss 0.205176\n",
      "batch 551: loss 0.161863\n",
      "batch 552: loss 0.195529\n",
      "batch 553: loss 0.206218\n",
      "batch 554: loss 0.240097\n",
      "batch 555: loss 0.206348\n",
      "batch 556: loss 0.273236\n",
      "batch 557: loss 0.165416\n",
      "batch 558: loss 0.224683\n",
      "batch 559: loss 0.175363\n",
      "batch 560: loss 0.128041\n",
      "batch 561: loss 0.205641\n",
      "batch 562: loss 0.156999\n",
      "batch 563: loss 0.258928\n",
      "batch 564: loss 0.300400\n",
      "batch 565: loss 0.161065\n",
      "batch 566: loss 0.149413\n",
      "batch 567: loss 0.139974\n",
      "batch 568: loss 0.105937\n",
      "batch 569: loss 0.142352\n",
      "batch 570: loss 0.245180\n",
      "batch 571: loss 0.197979\n",
      "batch 572: loss 0.266455\n",
      "batch 573: loss 0.169148\n",
      "batch 574: loss 0.218543\n",
      "batch 575: loss 0.311218\n",
      "batch 576: loss 0.171599\n",
      "batch 577: loss 0.132015\n",
      "batch 578: loss 0.125397\n",
      "batch 579: loss 0.314674\n",
      "batch 580: loss 0.236462\n",
      "batch 581: loss 0.189477\n",
      "batch 582: loss 0.288483\n",
      "batch 583: loss 0.246181\n",
      "batch 584: loss 0.140950\n",
      "batch 585: loss 0.221401\n",
      "batch 586: loss 0.206812\n",
      "batch 587: loss 0.221088\n",
      "batch 588: loss 0.347230\n",
      "batch 589: loss 0.200477\n",
      "batch 590: loss 0.224405\n",
      "batch 591: loss 0.238649\n",
      "batch 592: loss 0.200217\n",
      "batch 593: loss 0.345858\n",
      "batch 594: loss 0.215653\n",
      "batch 595: loss 0.219914\n",
      "batch 596: loss 0.134684\n",
      "batch 597: loss 0.124747\n",
      "batch 598: loss 0.297759\n",
      "batch 599: loss 0.183934\n"
     ]
    }
   ],
   "source": [
    "num_batches = int(data_loader.num_train_data // batch_size * num_epochs)\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.945500\n"
     ]
    }
   ],
   "source": [
    "sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "num_batches = int(data_loader.num_test_data // batch_size)\n",
    "for batch_index in range(num_batches):\n",
    "    start_index, end_index = batch_index * batch_size, (batch_index + 1) * batch_size\n",
    "    y_pred = model.predict(data_loader.test_data[start_index: end_index])\n",
    "    sparse_categorical_accuracy.update_state(y_true=data_loader.test_label[start_index: end_index], y_pred=y_pred)\n",
    "print(\"test accuracy: %f\" % sparse_categorical_accuracy.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
