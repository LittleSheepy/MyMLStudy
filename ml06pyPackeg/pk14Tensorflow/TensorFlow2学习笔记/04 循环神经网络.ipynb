{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self):\n",
    "        path = tf.keras.utils.get_file('nietzsche.txt',\n",
    "            origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "        with open(path, encoding='utf-8') as f:\n",
    "            self.raw_text = f.read().lower()\n",
    "        self.chars = sorted(list(set(self.raw_text)))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "        self.text = [self.char_indices[c] for c in self.raw_text]\n",
    "\n",
    "    def get_batch(self, seq_length, batch_size):\n",
    "        seq = []\n",
    "        next_char = []\n",
    "        for i in range(batch_size):\n",
    "            index = np.random.randint(0, len(self.text) - seq_length)\n",
    "            seq.append(self.text[index:index+seq_length])\n",
    "            next_char.append(self.text[index+seq_length])\n",
    "        return np.array(seq), np.array(next_char)       # [batch_size, seq_length], [num_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.DataLoader at 0x1bad9a2b2b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(tf.keras.Model):\n",
    "    def __init__(self, num_chars, batch_size, seq_length):\n",
    "        super().__init__()\n",
    "        self.num_chars = num_chars\n",
    "        self.seq_length = seq_length\n",
    "        self.batch_size = batch_size\n",
    "        self.cell = tf.keras.layers.LSTMCell(units=256)\n",
    "        self.dense = tf.keras.layers.Dense(units=self.num_chars)\n",
    "\n",
    "    def call(self, inputs, from_logits=False):\n",
    "        inputs = tf.one_hot(inputs, depth=self.num_chars)       # [batch_size, seq_length, num_chars]\n",
    "        state = self.cell.get_initial_state(batch_size=self.batch_size, dtype=tf.float32)\n",
    "        for t in range(self.seq_length):\n",
    "            output, state = self.cell(inputs[:, t, :], state)\n",
    "        logits = self.dense(output)\n",
    "        if from_logits:\n",
    "            return logits\n",
    "        else:\n",
    "            return tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = 1000\n",
    "seq_length = 40\n",
    "batch_size = 50\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: loss 4.038029\n",
      "batch 1: loss 4.018785\n",
      "batch 2: loss 3.994541\n",
      "batch 3: loss 3.967263\n",
      "batch 4: loss 3.877993\n",
      "batch 5: loss 3.786879\n",
      "batch 6: loss 3.376133\n",
      "batch 7: loss 3.591201\n",
      "batch 8: loss 3.229809\n",
      "batch 9: loss 3.399043\n",
      "batch 10: loss 3.204137\n",
      "batch 11: loss 3.046906\n",
      "batch 12: loss 3.152682\n",
      "batch 13: loss 3.080732\n",
      "batch 14: loss 2.969451\n",
      "batch 15: loss 3.026066\n",
      "batch 16: loss 3.051837\n",
      "batch 17: loss 3.345060\n",
      "batch 18: loss 2.978452\n",
      "batch 19: loss 3.398625\n",
      "batch 20: loss 3.050918\n",
      "batch 21: loss 3.316294\n",
      "batch 22: loss 3.015188\n",
      "batch 23: loss 2.935402\n",
      "batch 24: loss 3.020400\n",
      "batch 25: loss 3.273469\n",
      "batch 26: loss 3.168616\n",
      "batch 27: loss 3.049195\n",
      "batch 28: loss 3.075008\n",
      "batch 29: loss 2.872312\n",
      "batch 30: loss 3.046339\n",
      "batch 31: loss 3.127310\n",
      "batch 32: loss 3.117383\n",
      "batch 33: loss 3.055665\n",
      "batch 34: loss 3.152632\n",
      "batch 35: loss 3.007311\n",
      "batch 36: loss 3.040401\n",
      "batch 37: loss 2.981986\n",
      "batch 38: loss 3.292441\n",
      "batch 39: loss 3.010469\n",
      "batch 40: loss 3.232718\n",
      "batch 41: loss 2.947930\n",
      "batch 42: loss 3.375290\n",
      "batch 43: loss 2.996278\n",
      "batch 44: loss 3.096850\n",
      "batch 45: loss 2.921638\n",
      "batch 46: loss 2.960948\n",
      "batch 47: loss 2.981536\n",
      "batch 48: loss 2.872901\n",
      "batch 49: loss 2.994128\n",
      "batch 50: loss 3.154724\n",
      "batch 51: loss 3.028088\n",
      "batch 52: loss 2.699634\n",
      "batch 53: loss 2.938752\n",
      "batch 54: loss 2.968665\n",
      "batch 55: loss 2.969073\n",
      "batch 56: loss 2.989229\n",
      "batch 57: loss 3.425869\n",
      "batch 58: loss 2.893487\n",
      "batch 59: loss 3.012895\n",
      "batch 60: loss 3.029131\n",
      "batch 61: loss 3.099720\n",
      "batch 62: loss 3.107156\n",
      "batch 63: loss 3.012138\n",
      "batch 64: loss 2.966152\n",
      "batch 65: loss 2.879451\n",
      "batch 66: loss 2.851374\n",
      "batch 67: loss 2.795835\n",
      "batch 68: loss 3.057130\n",
      "batch 69: loss 3.024125\n",
      "batch 70: loss 3.105032\n",
      "batch 71: loss 3.100405\n",
      "batch 72: loss 2.974081\n",
      "batch 73: loss 2.737943\n",
      "batch 74: loss 3.039958\n",
      "batch 75: loss 2.671611\n",
      "batch 76: loss 2.922357\n",
      "batch 77: loss 2.894062\n",
      "batch 78: loss 3.136244\n",
      "batch 79: loss 3.044813\n",
      "batch 80: loss 2.969370\n",
      "batch 81: loss 2.913812\n",
      "batch 82: loss 2.752298\n",
      "batch 83: loss 2.870672\n",
      "batch 84: loss 2.746841\n",
      "batch 85: loss 3.360652\n",
      "batch 86: loss 2.779656\n",
      "batch 87: loss 3.058045\n",
      "batch 88: loss 3.116573\n",
      "batch 89: loss 3.059051\n",
      "batch 90: loss 3.423888\n",
      "batch 91: loss 2.893116\n",
      "batch 92: loss 2.931627\n",
      "batch 93: loss 2.886477\n",
      "batch 94: loss 3.242671\n",
      "batch 95: loss 3.066796\n",
      "batch 96: loss 3.031265\n",
      "batch 97: loss 2.818227\n",
      "batch 98: loss 3.119033\n",
      "batch 99: loss 2.887353\n",
      "batch 100: loss 2.927066\n",
      "batch 101: loss 2.913741\n",
      "batch 102: loss 3.148901\n",
      "batch 103: loss 2.781570\n",
      "batch 104: loss 3.068721\n",
      "batch 105: loss 3.132870\n",
      "batch 106: loss 3.062432\n",
      "batch 107: loss 2.998985\n",
      "batch 108: loss 3.078095\n",
      "batch 109: loss 3.052609\n",
      "batch 110: loss 2.875306\n",
      "batch 111: loss 2.989302\n",
      "batch 112: loss 2.907843\n",
      "batch 113: loss 3.055490\n",
      "batch 114: loss 3.036078\n",
      "batch 115: loss 2.989049\n",
      "batch 116: loss 2.944470\n",
      "batch 117: loss 3.034132\n",
      "batch 118: loss 2.809187\n",
      "batch 119: loss 3.241613\n",
      "batch 120: loss 3.042360\n",
      "batch 121: loss 3.074375\n",
      "batch 122: loss 3.011031\n",
      "batch 123: loss 3.002620\n",
      "batch 124: loss 2.912974\n",
      "batch 125: loss 3.084317\n",
      "batch 126: loss 3.014698\n",
      "batch 127: loss 2.945524\n",
      "batch 128: loss 2.836732\n",
      "batch 129: loss 2.874157\n",
      "batch 130: loss 3.289807\n",
      "batch 131: loss 2.815670\n",
      "batch 132: loss 3.005784\n",
      "batch 133: loss 2.727909\n",
      "batch 134: loss 3.267241\n",
      "batch 135: loss 3.063680\n",
      "batch 136: loss 2.790304\n",
      "batch 137: loss 3.070180\n",
      "batch 138: loss 2.885238\n",
      "batch 139: loss 2.996850\n",
      "batch 140: loss 3.064617\n",
      "batch 141: loss 3.010039\n",
      "batch 142: loss 2.975284\n",
      "batch 143: loss 3.064876\n",
      "batch 144: loss 2.798684\n",
      "batch 145: loss 2.978388\n",
      "batch 146: loss 2.987495\n",
      "batch 147: loss 2.868204\n",
      "batch 148: loss 2.974821\n",
      "batch 149: loss 2.909397\n",
      "batch 150: loss 2.926652\n",
      "batch 151: loss 3.002868\n",
      "batch 152: loss 3.022320\n",
      "batch 153: loss 2.755158\n",
      "batch 154: loss 2.992619\n",
      "batch 155: loss 2.927148\n",
      "batch 156: loss 2.981653\n",
      "batch 157: loss 2.878018\n",
      "batch 158: loss 3.032088\n",
      "batch 159: loss 3.170774\n",
      "batch 160: loss 3.082707\n",
      "batch 161: loss 3.052365\n",
      "batch 162: loss 2.916420\n",
      "batch 163: loss 3.021942\n",
      "batch 164: loss 3.152906\n",
      "batch 165: loss 2.886891\n",
      "batch 166: loss 2.733484\n",
      "batch 167: loss 3.009635\n",
      "batch 168: loss 2.935992\n",
      "batch 169: loss 2.861100\n",
      "batch 170: loss 2.965598\n",
      "batch 171: loss 2.981867\n",
      "batch 172: loss 2.911935\n",
      "batch 173: loss 3.011163\n",
      "batch 174: loss 2.803252\n",
      "batch 175: loss 3.115940\n",
      "batch 176: loss 2.913251\n",
      "batch 177: loss 3.004007\n",
      "batch 178: loss 2.675247\n",
      "batch 179: loss 2.966191\n",
      "batch 180: loss 2.952872\n",
      "batch 181: loss 2.926363\n",
      "batch 182: loss 2.849324\n",
      "batch 183: loss 3.182401\n",
      "batch 184: loss 2.923111\n",
      "batch 185: loss 3.144658\n",
      "batch 186: loss 3.264341\n",
      "batch 187: loss 2.984868\n",
      "batch 188: loss 3.004642\n",
      "batch 189: loss 3.066620\n",
      "batch 190: loss 3.105517\n",
      "batch 191: loss 2.842569\n",
      "batch 192: loss 2.824188\n",
      "batch 193: loss 3.036813\n",
      "batch 194: loss 3.160190\n",
      "batch 195: loss 3.193820\n",
      "batch 196: loss 3.150407\n",
      "batch 197: loss 2.732131\n",
      "batch 198: loss 2.985201\n",
      "batch 199: loss 3.005048\n",
      "batch 200: loss 2.836011\n",
      "batch 201: loss 2.915103\n",
      "batch 202: loss 2.834222\n",
      "batch 203: loss 2.752588\n",
      "batch 204: loss 2.879149\n",
      "batch 205: loss 2.949837\n",
      "batch 206: loss 3.030677\n",
      "batch 207: loss 3.051758\n",
      "batch 208: loss 2.990562\n",
      "batch 209: loss 2.914548\n",
      "batch 210: loss 2.971953\n",
      "batch 211: loss 2.958568\n",
      "batch 212: loss 2.901994\n",
      "batch 213: loss 3.142506\n",
      "batch 214: loss 2.783547\n",
      "batch 215: loss 2.952445\n",
      "batch 216: loss 3.037671\n",
      "batch 217: loss 2.810978\n",
      "batch 218: loss 3.115826\n",
      "batch 219: loss 2.976165\n",
      "batch 220: loss 2.788531\n",
      "batch 221: loss 3.111449\n",
      "batch 222: loss 2.941825\n",
      "batch 223: loss 2.868345\n",
      "batch 224: loss 2.958713\n",
      "batch 225: loss 2.646179\n",
      "batch 226: loss 3.147699\n",
      "batch 227: loss 2.863703\n",
      "batch 228: loss 3.053414\n",
      "batch 229: loss 2.921815\n",
      "batch 230: loss 2.875101\n",
      "batch 231: loss 2.904078\n",
      "batch 232: loss 2.762017\n",
      "batch 233: loss 3.012533\n",
      "batch 234: loss 2.663373\n",
      "batch 235: loss 3.275879\n",
      "batch 236: loss 2.919638\n",
      "batch 237: loss 3.149127\n",
      "batch 238: loss 2.765675\n",
      "batch 239: loss 2.967549\n",
      "batch 240: loss 2.599490\n",
      "batch 241: loss 2.951810\n",
      "batch 242: loss 2.660744\n",
      "batch 243: loss 2.804226\n",
      "batch 244: loss 2.982753\n",
      "batch 245: loss 2.874167\n",
      "batch 246: loss 2.819433\n",
      "batch 247: loss 2.886410\n",
      "batch 248: loss 2.597502\n",
      "batch 249: loss 3.102823\n",
      "batch 250: loss 2.719426\n",
      "batch 251: loss 3.016891\n",
      "batch 252: loss 2.937322\n",
      "batch 253: loss 2.999865\n",
      "batch 254: loss 2.874745\n",
      "batch 255: loss 2.930265\n",
      "batch 256: loss 3.155113\n",
      "batch 257: loss 3.235809\n",
      "batch 258: loss 2.907536\n",
      "batch 259: loss 2.764672\n",
      "batch 260: loss 2.859641\n",
      "batch 261: loss 2.948057\n",
      "batch 262: loss 2.931031\n",
      "batch 263: loss 3.028116\n",
      "batch 264: loss 2.814896\n",
      "batch 265: loss 2.961441\n",
      "batch 266: loss 2.702864\n",
      "batch 267: loss 3.051700\n",
      "batch 268: loss 2.812355\n",
      "batch 269: loss 2.825283\n",
      "batch 270: loss 2.902166\n",
      "batch 271: loss 2.773055\n",
      "batch 272: loss 2.982566\n",
      "batch 273: loss 3.043662\n",
      "batch 274: loss 2.823455\n",
      "batch 275: loss 2.895710\n",
      "batch 276: loss 2.757473\n",
      "batch 277: loss 2.814109\n",
      "batch 278: loss 2.926246\n",
      "batch 279: loss 2.991675\n",
      "batch 280: loss 2.835787\n",
      "batch 281: loss 2.985917\n",
      "batch 282: loss 2.797865\n",
      "batch 283: loss 2.961169\n",
      "batch 284: loss 3.072783\n",
      "batch 285: loss 2.752981\n",
      "batch 286: loss 3.062490\n",
      "batch 287: loss 2.941693\n",
      "batch 288: loss 2.569198\n",
      "batch 289: loss 2.835389\n",
      "batch 290: loss 2.693722\n",
      "batch 291: loss 2.980451\n",
      "batch 292: loss 3.180880\n",
      "batch 293: loss 3.088813\n",
      "batch 294: loss 2.760741\n",
      "batch 295: loss 2.838848\n",
      "batch 296: loss 2.850388\n",
      "batch 297: loss 2.847405\n",
      "batch 298: loss 2.904687\n",
      "batch 299: loss 3.057902\n",
      "batch 300: loss 2.763544\n",
      "batch 301: loss 2.873027\n",
      "batch 302: loss 2.625176\n",
      "batch 303: loss 2.660632\n",
      "batch 304: loss 2.887882\n",
      "batch 305: loss 2.883087\n",
      "batch 306: loss 3.109525\n",
      "batch 307: loss 2.760849\n",
      "batch 308: loss 2.846830\n",
      "batch 309: loss 2.840693\n",
      "batch 310: loss 2.898912\n",
      "batch 311: loss 2.946934\n",
      "batch 312: loss 2.790051\n",
      "batch 313: loss 2.711344\n",
      "batch 314: loss 2.754728\n",
      "batch 315: loss 3.301234\n",
      "batch 316: loss 2.537338\n",
      "batch 317: loss 2.820667\n",
      "batch 318: loss 2.763655\n",
      "batch 319: loss 3.122633\n",
      "batch 320: loss 2.624352\n",
      "batch 321: loss 2.835549\n",
      "batch 322: loss 2.932837\n",
      "batch 323: loss 3.123417\n",
      "batch 324: loss 2.673362\n",
      "batch 325: loss 2.840949\n",
      "batch 326: loss 2.682680\n",
      "batch 327: loss 2.936245\n",
      "batch 328: loss 2.647775\n",
      "batch 329: loss 2.631268\n",
      "batch 330: loss 2.636820\n",
      "batch 331: loss 2.987699\n",
      "batch 332: loss 2.725734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 333: loss 3.108130\n",
      "batch 334: loss 2.744048\n",
      "batch 335: loss 2.788225\n",
      "batch 336: loss 2.667076\n",
      "batch 337: loss 2.925707\n",
      "batch 338: loss 2.795533\n",
      "batch 339: loss 2.936799\n",
      "batch 340: loss 2.808050\n",
      "batch 341: loss 2.669323\n",
      "batch 342: loss 2.923522\n",
      "batch 343: loss 2.745440\n",
      "batch 344: loss 2.651659\n",
      "batch 345: loss 2.794531\n",
      "batch 346: loss 2.672913\n",
      "batch 347: loss 3.174884\n",
      "batch 348: loss 2.731817\n",
      "batch 349: loss 2.667836\n",
      "batch 350: loss 2.915717\n",
      "batch 351: loss 2.941373\n",
      "batch 352: loss 2.621330\n",
      "batch 353: loss 2.724606\n",
      "batch 354: loss 2.753261\n",
      "batch 355: loss 3.008926\n",
      "batch 356: loss 2.801748\n",
      "batch 357: loss 2.657449\n",
      "batch 358: loss 2.702275\n",
      "batch 359: loss 2.825028\n",
      "batch 360: loss 2.665081\n",
      "batch 361: loss 3.139986\n",
      "batch 362: loss 2.719180\n",
      "batch 363: loss 3.194463\n",
      "batch 364: loss 2.541144\n",
      "batch 365: loss 2.637795\n",
      "batch 366: loss 2.987697\n",
      "batch 367: loss 2.766572\n",
      "batch 368: loss 2.837399\n",
      "batch 369: loss 2.794475\n",
      "batch 370: loss 2.974923\n",
      "batch 371: loss 2.923979\n",
      "batch 372: loss 2.836269\n",
      "batch 373: loss 2.825338\n",
      "batch 374: loss 2.950582\n",
      "batch 375: loss 2.895386\n",
      "batch 376: loss 2.758928\n",
      "batch 377: loss 3.007170\n",
      "batch 378: loss 2.676758\n",
      "batch 379: loss 2.689883\n",
      "batch 380: loss 2.910081\n",
      "batch 381: loss 2.685464\n",
      "batch 382: loss 2.788552\n",
      "batch 383: loss 2.883637\n",
      "batch 384: loss 2.685967\n",
      "batch 385: loss 2.903171\n",
      "batch 386: loss 2.405650\n",
      "batch 387: loss 2.512849\n",
      "batch 388: loss 2.635354\n",
      "batch 389: loss 2.575811\n",
      "batch 390: loss 3.090112\n",
      "batch 391: loss 2.519054\n",
      "batch 392: loss 2.726376\n",
      "batch 393: loss 2.500295\n",
      "batch 394: loss 2.928249\n",
      "batch 395: loss 2.714826\n",
      "batch 396: loss 2.413944\n",
      "batch 397: loss 2.807816\n",
      "batch 398: loss 2.625472\n",
      "batch 399: loss 2.767356\n",
      "batch 400: loss 2.769921\n",
      "batch 401: loss 2.680393\n",
      "batch 402: loss 2.764991\n",
      "batch 403: loss 2.824908\n",
      "batch 404: loss 2.690900\n",
      "batch 405: loss 2.974040\n",
      "batch 406: loss 2.852045\n",
      "batch 407: loss 2.731632\n",
      "batch 408: loss 2.794979\n",
      "batch 409: loss 2.739397\n",
      "batch 410: loss 2.638957\n",
      "batch 411: loss 2.607544\n",
      "batch 412: loss 2.879131\n",
      "batch 413: loss 2.499281\n",
      "batch 414: loss 2.393296\n",
      "batch 415: loss 2.744843\n",
      "batch 416: loss 2.605028\n",
      "batch 417: loss 2.688359\n",
      "batch 418: loss 2.587306\n",
      "batch 419: loss 2.959852\n",
      "batch 420: loss 2.649153\n",
      "batch 421: loss 2.596948\n",
      "batch 422: loss 2.573252\n",
      "batch 423: loss 2.501884\n",
      "batch 424: loss 2.468576\n",
      "batch 425: loss 2.654597\n",
      "batch 426: loss 2.558173\n",
      "batch 427: loss 2.654907\n",
      "batch 428: loss 2.393334\n",
      "batch 429: loss 2.902681\n",
      "batch 430: loss 2.877315\n",
      "batch 431: loss 3.025984\n",
      "batch 432: loss 2.694268\n",
      "batch 433: loss 2.787492\n",
      "batch 434: loss 2.702228\n",
      "batch 435: loss 2.454160\n",
      "batch 436: loss 2.533365\n",
      "batch 437: loss 2.742623\n",
      "batch 438: loss 2.241201\n",
      "batch 439: loss 2.856812\n",
      "batch 440: loss 2.515730\n",
      "batch 441: loss 2.741847\n",
      "batch 442: loss 2.549091\n",
      "batch 443: loss 2.824508\n",
      "batch 444: loss 2.703592\n",
      "batch 445: loss 2.583864\n",
      "batch 446: loss 2.788950\n",
      "batch 447: loss 2.590186\n",
      "batch 448: loss 2.729615\n",
      "batch 449: loss 2.529474\n",
      "batch 450: loss 2.603827\n",
      "batch 451: loss 2.466378\n",
      "batch 452: loss 2.718063\n",
      "batch 453: loss 2.409950\n",
      "batch 454: loss 2.645972\n",
      "batch 455: loss 2.623616\n",
      "batch 456: loss 2.517095\n",
      "batch 457: loss 2.740686\n",
      "batch 458: loss 2.548439\n",
      "batch 459: loss 2.568830\n",
      "batch 460: loss 3.116529\n",
      "batch 461: loss 2.785416\n",
      "batch 462: loss 2.716565\n",
      "batch 463: loss 2.776323\n",
      "batch 464: loss 2.651793\n",
      "batch 465: loss 2.495597\n",
      "batch 466: loss 2.384436\n",
      "batch 467: loss 2.574027\n",
      "batch 468: loss 2.460142\n",
      "batch 469: loss 2.500105\n",
      "batch 470: loss 2.836957\n",
      "batch 471: loss 2.534631\n",
      "batch 472: loss 2.771043\n",
      "batch 473: loss 2.556240\n",
      "batch 474: loss 2.536672\n",
      "batch 475: loss 2.652334\n",
      "batch 476: loss 2.649021\n",
      "batch 477: loss 2.611620\n",
      "batch 478: loss 2.932827\n",
      "batch 479: loss 2.618277\n",
      "batch 480: loss 2.554010\n",
      "batch 481: loss 2.840827\n",
      "batch 482: loss 2.504059\n",
      "batch 483: loss 2.897888\n",
      "batch 484: loss 2.692973\n",
      "batch 485: loss 2.504663\n",
      "batch 486: loss 2.594441\n",
      "batch 487: loss 2.603414\n",
      "batch 488: loss 2.293869\n",
      "batch 489: loss 2.688980\n",
      "batch 490: loss 2.576171\n",
      "batch 491: loss 2.684659\n",
      "batch 492: loss 3.011153\n",
      "batch 493: loss 2.619266\n",
      "batch 494: loss 2.653347\n",
      "batch 495: loss 2.669484\n",
      "batch 496: loss 2.730542\n",
      "batch 497: loss 2.562454\n",
      "batch 498: loss 2.844575\n",
      "batch 499: loss 2.647057\n",
      "batch 500: loss 2.508495\n",
      "batch 501: loss 2.747828\n",
      "batch 502: loss 2.705821\n",
      "batch 503: loss 2.579513\n",
      "batch 504: loss 2.576890\n",
      "batch 505: loss 2.396987\n",
      "batch 506: loss 2.489100\n",
      "batch 507: loss 2.454723\n",
      "batch 508: loss 2.961806\n",
      "batch 509: loss 2.696000\n",
      "batch 510: loss 2.367663\n",
      "batch 511: loss 2.652605\n",
      "batch 512: loss 2.719268\n",
      "batch 513: loss 2.565159\n",
      "batch 514: loss 2.358089\n",
      "batch 515: loss 3.080332\n",
      "batch 516: loss 2.608327\n",
      "batch 517: loss 2.547333\n",
      "batch 518: loss 2.308722\n",
      "batch 519: loss 2.504426\n",
      "batch 520: loss 2.563048\n",
      "batch 521: loss 2.753702\n",
      "batch 522: loss 2.643917\n",
      "batch 523: loss 2.337882\n",
      "batch 524: loss 2.598360\n",
      "batch 525: loss 2.488271\n",
      "batch 526: loss 2.668210\n",
      "batch 527: loss 2.370323\n",
      "batch 528: loss 2.768952\n",
      "batch 529: loss 2.493685\n",
      "batch 530: loss 2.523489\n",
      "batch 531: loss 2.738097\n",
      "batch 532: loss 2.411111\n",
      "batch 533: loss 2.428645\n",
      "batch 534: loss 2.643569\n",
      "batch 535: loss 2.571557\n",
      "batch 536: loss 2.689697\n",
      "batch 537: loss 2.207983\n",
      "batch 538: loss 2.547123\n",
      "batch 539: loss 2.943406\n",
      "batch 540: loss 2.747511\n",
      "batch 541: loss 2.597778\n",
      "batch 542: loss 2.562942\n",
      "batch 543: loss 2.496055\n",
      "batch 544: loss 2.805199\n",
      "batch 545: loss 2.900977\n",
      "batch 546: loss 2.721259\n",
      "batch 547: loss 2.655389\n",
      "batch 548: loss 2.475338\n",
      "batch 549: loss 2.815733\n",
      "batch 550: loss 2.410321\n",
      "batch 551: loss 2.432510\n",
      "batch 552: loss 2.607194\n",
      "batch 553: loss 2.330599\n",
      "batch 554: loss 2.769949\n",
      "batch 555: loss 2.512504\n",
      "batch 556: loss 3.012488\n",
      "batch 557: loss 2.427638\n",
      "batch 558: loss 2.707198\n",
      "batch 559: loss 2.794066\n",
      "batch 560: loss 2.551959\n",
      "batch 561: loss 2.662308\n",
      "batch 562: loss 2.835285\n",
      "batch 563: loss 2.532309\n",
      "batch 564: loss 2.730195\n",
      "batch 565: loss 2.650529\n",
      "batch 566: loss 2.744978\n",
      "batch 567: loss 2.518327\n",
      "batch 568: loss 2.523836\n",
      "batch 569: loss 2.617365\n",
      "batch 570: loss 2.497800\n",
      "batch 571: loss 2.365951\n",
      "batch 572: loss 2.733599\n",
      "batch 573: loss 2.726468\n",
      "batch 574: loss 2.428056\n",
      "batch 575: loss 2.659477\n",
      "batch 576: loss 2.533636\n",
      "batch 577: loss 2.695674\n",
      "batch 578: loss 2.497229\n",
      "batch 579: loss 2.330462\n",
      "batch 580: loss 2.918370\n",
      "batch 581: loss 2.474752\n",
      "batch 582: loss 2.701166\n",
      "batch 583: loss 2.468635\n",
      "batch 584: loss 2.667043\n",
      "batch 585: loss 2.682872\n",
      "batch 586: loss 2.704333\n",
      "batch 587: loss 2.480408\n",
      "batch 588: loss 2.476003\n",
      "batch 589: loss 2.373350\n",
      "batch 590: loss 2.579064\n",
      "batch 591: loss 2.616836\n",
      "batch 592: loss 2.406103\n",
      "batch 593: loss 2.712100\n",
      "batch 594: loss 2.259420\n",
      "batch 595: loss 2.815049\n",
      "batch 596: loss 2.561446\n",
      "batch 597: loss 2.496653\n",
      "batch 598: loss 2.413353\n",
      "batch 599: loss 2.571944\n",
      "batch 600: loss 2.434666\n",
      "batch 601: loss 2.650853\n",
      "batch 602: loss 2.586170\n",
      "batch 603: loss 2.380549\n",
      "batch 604: loss 2.490944\n",
      "batch 605: loss 2.601076\n",
      "batch 606: loss 2.825300\n",
      "batch 607: loss 2.570120\n",
      "batch 608: loss 2.762918\n",
      "batch 609: loss 2.590514\n",
      "batch 610: loss 2.651303\n",
      "batch 611: loss 2.802428\n",
      "batch 612: loss 2.743562\n",
      "batch 613: loss 2.527702\n",
      "batch 614: loss 2.777333\n",
      "batch 615: loss 2.330374\n",
      "batch 616: loss 2.423162\n",
      "batch 617: loss 2.491488\n",
      "batch 618: loss 2.413813\n",
      "batch 619: loss 2.707822\n",
      "batch 620: loss 2.497973\n",
      "batch 621: loss 2.604955\n",
      "batch 622: loss 2.505875\n",
      "batch 623: loss 2.476428\n",
      "batch 624: loss 2.775061\n",
      "batch 625: loss 2.593792\n",
      "batch 626: loss 2.390067\n",
      "batch 627: loss 2.268171\n",
      "batch 628: loss 2.359681\n",
      "batch 629: loss 2.663092\n",
      "batch 630: loss 2.534651\n",
      "batch 631: loss 2.683252\n",
      "batch 632: loss 2.921864\n",
      "batch 633: loss 2.275959\n",
      "batch 634: loss 2.474690\n",
      "batch 635: loss 2.474383\n",
      "batch 636: loss 2.561324\n",
      "batch 637: loss 2.024767\n",
      "batch 638: loss 2.201502\n",
      "batch 639: loss 2.601841\n",
      "batch 640: loss 2.327927\n",
      "batch 641: loss 2.590089\n",
      "batch 642: loss 2.761587\n",
      "batch 643: loss 2.483749\n",
      "batch 644: loss 2.724549\n",
      "batch 645: loss 2.475998\n",
      "batch 646: loss 2.671309\n",
      "batch 647: loss 2.187971\n",
      "batch 648: loss 2.141417\n",
      "batch 649: loss 2.394662\n",
      "batch 650: loss 2.458135\n",
      "batch 651: loss 2.503456\n",
      "batch 652: loss 2.407997\n",
      "batch 653: loss 2.270185\n",
      "batch 654: loss 2.371621\n",
      "batch 655: loss 2.466038\n",
      "batch 656: loss 2.708997\n",
      "batch 657: loss 2.409600\n",
      "batch 658: loss 2.378278\n",
      "batch 659: loss 2.326378\n",
      "batch 660: loss 2.557683\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 661: loss 2.404526\n",
      "batch 662: loss 2.562866\n",
      "batch 663: loss 2.464996\n",
      "batch 664: loss 2.479268\n",
      "batch 665: loss 2.584188\n",
      "batch 666: loss 2.429455\n",
      "batch 667: loss 2.313926\n",
      "batch 668: loss 2.544755\n",
      "batch 669: loss 2.705447\n",
      "batch 670: loss 2.665357\n",
      "batch 671: loss 2.381393\n",
      "batch 672: loss 2.478146\n",
      "batch 673: loss 2.878117\n",
      "batch 674: loss 2.228875\n",
      "batch 675: loss 2.634471\n",
      "batch 676: loss 2.373473\n",
      "batch 677: loss 2.571218\n",
      "batch 678: loss 2.620503\n",
      "batch 679: loss 2.201135\n",
      "batch 680: loss 2.619467\n",
      "batch 681: loss 2.577143\n",
      "batch 682: loss 2.350256\n",
      "batch 683: loss 2.594701\n",
      "batch 684: loss 2.252181\n",
      "batch 685: loss 2.617167\n",
      "batch 686: loss 2.195329\n",
      "batch 687: loss 2.576474\n",
      "batch 688: loss 2.602294\n",
      "batch 689: loss 2.708124\n",
      "batch 690: loss 2.557644\n",
      "batch 691: loss 2.304030\n",
      "batch 692: loss 2.543303\n",
      "batch 693: loss 2.767826\n",
      "batch 694: loss 2.470592\n",
      "batch 695: loss 2.308557\n",
      "batch 696: loss 2.323673\n",
      "batch 697: loss 2.174050\n",
      "batch 698: loss 2.940674\n",
      "batch 699: loss 2.347711\n",
      "batch 700: loss 2.230034\n",
      "batch 701: loss 2.283617\n",
      "batch 702: loss 2.244583\n",
      "batch 703: loss 2.654606\n",
      "batch 704: loss 2.418061\n",
      "batch 705: loss 2.245897\n",
      "batch 706: loss 2.233579\n",
      "batch 707: loss 2.502464\n",
      "batch 708: loss 2.396343\n",
      "batch 709: loss 2.386604\n",
      "batch 710: loss 2.500518\n",
      "batch 711: loss 2.476100\n",
      "batch 712: loss 2.644612\n",
      "batch 713: loss 2.312331\n",
      "batch 714: loss 2.409901\n",
      "batch 715: loss 2.675462\n",
      "batch 716: loss 2.286815\n",
      "batch 717: loss 2.516318\n",
      "batch 718: loss 2.387492\n",
      "batch 719: loss 2.578046\n",
      "batch 720: loss 2.514957\n",
      "batch 721: loss 2.266342\n",
      "batch 722: loss 2.635891\n",
      "batch 723: loss 2.719003\n",
      "batch 724: loss 2.624976\n",
      "batch 725: loss 2.678734\n",
      "batch 726: loss 2.601618\n",
      "batch 727: loss 2.707070\n",
      "batch 728: loss 2.538470\n",
      "batch 729: loss 2.676412\n",
      "batch 730: loss 2.674390\n",
      "batch 731: loss 2.379449\n",
      "batch 732: loss 2.438539\n",
      "batch 733: loss 2.584391\n",
      "batch 734: loss 2.421933\n",
      "batch 735: loss 2.580055\n",
      "batch 736: loss 2.666647\n",
      "batch 737: loss 2.619418\n",
      "batch 738: loss 2.470031\n",
      "batch 739: loss 2.733864\n",
      "batch 740: loss 2.401437\n",
      "batch 741: loss 2.616688\n",
      "batch 742: loss 2.262113\n",
      "batch 743: loss 2.527893\n",
      "batch 744: loss 2.806887\n",
      "batch 745: loss 2.356292\n",
      "batch 746: loss 2.445266\n",
      "batch 747: loss 2.599965\n",
      "batch 748: loss 2.614410\n",
      "batch 749: loss 2.667632\n",
      "batch 750: loss 2.470535\n",
      "batch 751: loss 2.355019\n",
      "batch 752: loss 2.444881\n",
      "batch 753: loss 2.599409\n",
      "batch 754: loss 2.169633\n",
      "batch 755: loss 2.347896\n",
      "batch 756: loss 2.639132\n",
      "batch 757: loss 2.404738\n",
      "batch 758: loss 2.160986\n",
      "batch 759: loss 2.561275\n",
      "batch 760: loss 2.275695\n",
      "batch 761: loss 3.007677\n",
      "batch 762: loss 2.389631\n",
      "batch 763: loss 2.486974\n",
      "batch 764: loss 2.620243\n",
      "batch 765: loss 2.565366\n",
      "batch 766: loss 2.474861\n",
      "batch 767: loss 2.571943\n",
      "batch 768: loss 2.639395\n",
      "batch 769: loss 2.283444\n",
      "batch 770: loss 2.277432\n",
      "batch 771: loss 2.360130\n",
      "batch 772: loss 2.575024\n",
      "batch 773: loss 2.542801\n",
      "batch 774: loss 2.558722\n",
      "batch 775: loss 2.745893\n",
      "batch 776: loss 2.276341\n",
      "batch 777: loss 2.357002\n",
      "batch 778: loss 2.794811\n",
      "batch 779: loss 2.264671\n",
      "batch 780: loss 2.488471\n",
      "batch 781: loss 2.647414\n",
      "batch 782: loss 2.159791\n",
      "batch 783: loss 2.587963\n",
      "batch 784: loss 2.144465\n",
      "batch 785: loss 2.510596\n",
      "batch 786: loss 2.430964\n",
      "batch 787: loss 2.654103\n",
      "batch 788: loss 2.492173\n",
      "batch 789: loss 2.518780\n",
      "batch 790: loss 2.480265\n",
      "batch 791: loss 2.376744\n",
      "batch 792: loss 2.507834\n",
      "batch 793: loss 2.687388\n",
      "batch 794: loss 2.472358\n",
      "batch 795: loss 2.237987\n",
      "batch 796: loss 2.440157\n",
      "batch 797: loss 2.616803\n",
      "batch 798: loss 2.552303\n",
      "batch 799: loss 2.401589\n",
      "batch 800: loss 2.140026\n",
      "batch 801: loss 2.651318\n",
      "batch 802: loss 2.439188\n",
      "batch 803: loss 2.407832\n",
      "batch 804: loss 2.284884\n",
      "batch 805: loss 2.145562\n",
      "batch 806: loss 2.836187\n",
      "batch 807: loss 2.781383\n",
      "batch 808: loss 2.427183\n",
      "batch 809: loss 2.251832\n",
      "batch 810: loss 2.508654\n",
      "batch 811: loss 2.399602\n",
      "batch 812: loss 2.690107\n",
      "batch 813: loss 2.471821\n",
      "batch 814: loss 2.484308\n",
      "batch 815: loss 2.396476\n",
      "batch 816: loss 2.212438\n",
      "batch 817: loss 2.337868\n",
      "batch 818: loss 2.381378\n",
      "batch 819: loss 2.342373\n",
      "batch 820: loss 2.253990\n",
      "batch 821: loss 2.284548\n",
      "batch 822: loss 2.414055\n",
      "batch 823: loss 2.175216\n",
      "batch 824: loss 2.043051\n",
      "batch 825: loss 2.900532\n",
      "batch 826: loss 2.248438\n",
      "batch 827: loss 2.029579\n",
      "batch 828: loss 2.294891\n",
      "batch 829: loss 2.549383\n",
      "batch 830: loss 2.489476\n",
      "batch 831: loss 2.421302\n",
      "batch 832: loss 2.478599\n",
      "batch 833: loss 2.227910\n",
      "batch 834: loss 2.638486\n",
      "batch 835: loss 2.288669\n",
      "batch 836: loss 2.829549\n",
      "batch 837: loss 2.099500\n",
      "batch 838: loss 2.189286\n",
      "batch 839: loss 2.460575\n",
      "batch 840: loss 2.848015\n",
      "batch 841: loss 2.094642\n",
      "batch 842: loss 2.313594\n",
      "batch 843: loss 2.561276\n",
      "batch 844: loss 2.447178\n",
      "batch 845: loss 2.463560\n",
      "batch 846: loss 2.283644\n",
      "batch 847: loss 2.613247\n",
      "batch 848: loss 2.149493\n",
      "batch 849: loss 2.369921\n",
      "batch 850: loss 2.547499\n",
      "batch 851: loss 2.392644\n",
      "batch 852: loss 2.540286\n",
      "batch 853: loss 2.320033\n",
      "batch 854: loss 2.386581\n",
      "batch 855: loss 2.581925\n",
      "batch 856: loss 2.425035\n",
      "batch 857: loss 2.295076\n",
      "batch 858: loss 2.598777\n",
      "batch 859: loss 2.775356\n",
      "batch 860: loss 2.225327\n",
      "batch 861: loss 2.624378\n",
      "batch 862: loss 2.504523\n",
      "batch 863: loss 2.202350\n",
      "batch 864: loss 2.250391\n",
      "batch 865: loss 2.218772\n",
      "batch 866: loss 2.625910\n",
      "batch 867: loss 2.411535\n",
      "batch 868: loss 2.479831\n",
      "batch 869: loss 2.437998\n",
      "batch 870: loss 2.218316\n",
      "batch 871: loss 2.477275\n",
      "batch 872: loss 2.566133\n",
      "batch 873: loss 2.530080\n",
      "batch 874: loss 2.626128\n",
      "batch 875: loss 2.254742\n",
      "batch 876: loss 2.335099\n",
      "batch 877: loss 2.586082\n",
      "batch 878: loss 2.374279\n",
      "batch 879: loss 2.480407\n",
      "batch 880: loss 2.542401\n",
      "batch 881: loss 2.391956\n",
      "batch 882: loss 2.514652\n",
      "batch 883: loss 2.439038\n",
      "batch 884: loss 2.149291\n",
      "batch 885: loss 2.231227\n",
      "batch 886: loss 2.458993\n",
      "batch 887: loss 2.486695\n",
      "batch 888: loss 2.469265\n",
      "batch 889: loss 2.428289\n",
      "batch 890: loss 2.833992\n",
      "batch 891: loss 2.693529\n",
      "batch 892: loss 2.191752\n",
      "batch 893: loss 2.630519\n",
      "batch 894: loss 2.620371\n",
      "batch 895: loss 2.737477\n",
      "batch 896: loss 2.557537\n",
      "batch 897: loss 2.412106\n",
      "batch 898: loss 2.498293\n",
      "batch 899: loss 2.318331\n",
      "batch 900: loss 2.488503\n",
      "batch 901: loss 2.391271\n",
      "batch 902: loss 2.504591\n",
      "batch 903: loss 2.558320\n",
      "batch 904: loss 2.542065\n",
      "batch 905: loss 2.458841\n",
      "batch 906: loss 2.219531\n",
      "batch 907: loss 2.516506\n",
      "batch 908: loss 2.429404\n",
      "batch 909: loss 2.385105\n",
      "batch 910: loss 2.394114\n",
      "batch 911: loss 2.563426\n",
      "batch 912: loss 2.818735\n",
      "batch 913: loss 2.652553\n",
      "batch 914: loss 2.601079\n",
      "batch 915: loss 2.502688\n",
      "batch 916: loss 2.444250\n",
      "batch 917: loss 2.364552\n",
      "batch 918: loss 2.390822\n",
      "batch 919: loss 2.485742\n",
      "batch 920: loss 2.325466\n",
      "batch 921: loss 2.475798\n",
      "batch 922: loss 2.607054\n",
      "batch 923: loss 2.389540\n",
      "batch 924: loss 2.533725\n",
      "batch 925: loss 2.416566\n",
      "batch 926: loss 2.077794\n",
      "batch 927: loss 2.609782\n",
      "batch 928: loss 2.240454\n",
      "batch 929: loss 2.536678\n",
      "batch 930: loss 2.699327\n",
      "batch 931: loss 2.563209\n",
      "batch 932: loss 2.279038\n",
      "batch 933: loss 2.500027\n",
      "batch 934: loss 2.281708\n",
      "batch 935: loss 2.533863\n",
      "batch 936: loss 2.369537\n",
      "batch 937: loss 2.622892\n",
      "batch 938: loss 2.291032\n",
      "batch 939: loss 2.369088\n",
      "batch 940: loss 2.578049\n",
      "batch 941: loss 2.164982\n",
      "batch 942: loss 2.395184\n",
      "batch 943: loss 2.394497\n",
      "batch 944: loss 2.640839\n",
      "batch 945: loss 2.556805\n",
      "batch 946: loss 2.342368\n",
      "batch 947: loss 2.224682\n",
      "batch 948: loss 2.327662\n",
      "batch 949: loss 2.489828\n",
      "batch 950: loss 2.360174\n",
      "batch 951: loss 2.484622\n",
      "batch 952: loss 2.407206\n",
      "batch 953: loss 2.434523\n",
      "batch 954: loss 2.287164\n",
      "batch 955: loss 2.574409\n",
      "batch 956: loss 2.106535\n",
      "batch 957: loss 2.454372\n",
      "batch 958: loss 2.248944\n",
      "batch 959: loss 2.586927\n",
      "batch 960: loss 2.612622\n",
      "batch 961: loss 2.858450\n",
      "batch 962: loss 2.658932\n",
      "batch 963: loss 2.731541\n",
      "batch 964: loss 2.555680\n",
      "batch 965: loss 2.695404\n",
      "batch 966: loss 2.413368\n",
      "batch 967: loss 2.231611\n",
      "batch 968: loss 2.504576\n",
      "batch 969: loss 2.379948\n",
      "batch 970: loss 2.499993\n",
      "batch 971: loss 2.322922\n",
      "batch 972: loss 2.198407\n",
      "batch 973: loss 2.785358\n",
      "batch 974: loss 2.398635\n",
      "batch 975: loss 2.452130\n",
      "batch 976: loss 2.347898\n",
      "batch 977: loss 2.735008\n",
      "batch 978: loss 2.488084\n",
      "batch 979: loss 2.205280\n",
      "batch 980: loss 2.705298\n",
      "batch 981: loss 2.697158\n",
      "batch 982: loss 2.585648\n",
      "batch 983: loss 2.835182\n",
      "batch 984: loss 2.563790\n",
      "batch 985: loss 2.535187\n",
      "batch 986: loss 2.548168\n",
      "batch 987: loss 2.682149\n",
      "batch 988: loss 2.607333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 989: loss 2.180336\n",
      "batch 990: loss 2.004422\n",
      "batch 991: loss 2.216562\n",
      "batch 992: loss 2.503207\n",
      "batch 993: loss 2.183556\n",
      "batch 994: loss 1.993610\n",
      "batch 995: loss 2.401442\n",
      "batch 996: loss 2.321387\n",
      "batch 997: loss 2.302971\n",
      "batch 998: loss 2.549250\n",
      "batch 999: loss 2.462742\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataLoader()\n",
    "model = RNN(num_chars=len(data_loader.chars), batch_size=batch_size, seq_length=seq_length)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(seq_length, batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, inputs, temperature=1.):\n",
    "    batch_size, _ = tf.shape(inputs)\n",
    "    logits = self(inputs, from_logits=True)\n",
    "    prob = tf.nn.softmax(logits / temperature).numpy()\n",
    "    return np.array([np.random.choice(self.num_chars, p=prob[i, :])\n",
    "                     for i in range(batch_size.numpy())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diversity 0.200000:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "slice indices must be integers or None or have an __index__ method",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-952409d6c20c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"diversity %f:\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiversity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindices_char\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflush\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    907\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    908\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 909\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    910\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m         \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m         \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m         callbacks=callbacks)\n\u001b[0m",
      "\u001b[1;32mD:\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    360\u001b[0m       \u001b[0mbatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_batches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_samples_or_steps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch_start\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_end\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m         \u001b[0mbatch_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex_array\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbatch_start\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_end\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m         \u001b[1;31m# Slice into a batch.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: slice indices must be integers or None or have an __index__ method"
     ]
    }
   ],
   "source": [
    "X_, _ = data_loader.get_batch(seq_length, 1)\n",
    "for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "    X = X_\n",
    "    print(\"diversity %f:\" % diversity)\n",
    "    for t in range(400):\n",
    "        y_pred = model.predict(X, diversity)\n",
    "        print(data_loader.indices_char[y_pred[0]], end='', flush=True)\n",
    "        X = np.concatenate([X[:, 1:], np.expand_dims(y_pred, axis=1)], axis=-1)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
