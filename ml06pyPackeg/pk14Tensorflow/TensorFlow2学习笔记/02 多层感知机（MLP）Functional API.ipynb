{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据获取及预处理\n",
    "class MNISTLoader():\n",
    "    def __init__(self):\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()\n",
    "        # MNIST中的图像默认为uint8（0-255的数字）。以下代码将其归一化到0-1之间的浮点数，并在最后增加一维作为颜色通道\n",
    "        print(self.train_data.shape)\n",
    "        self.train_data = np.expand_dims(self.train_data.astype(np.float32) / 255.0, axis=-1)      # [60000, 28, 28, 1]\n",
    "        print(self.train_data.shape)\n",
    "        self.test_data = np.expand_dims(self.test_data.astype(np.float32) / 255.0, axis=-1)        # [10000, 28, 28, 1]\n",
    "        self.train_label = self.train_label.astype(np.int32)    # [60000]\n",
    "        self.test_label = self.test_label.astype(np.int32)      # [10000]\n",
    "        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        # 从数据集中随机取出batch_size个元素并返回\n",
    "        index = np.random.randint(0, np.shape(self.train_data)[0], batch_size)\n",
    "        return self.train_data[index, :], self.train_label[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "inputs = tf.keras.Input(shape=(28, 28, 1))\n",
    "x = tf.keras.layers.Flatten()(inputs)\n",
    "x = tf.keras.layers.Dense(units=100, activation=tf.nn.relu)(x)\n",
    "x = tf.keras.layers.Dense(units=10)(x)\n",
    "outputs = tf.keras.layers.Softmax()(x)\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "data_loader = MNISTLoader()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 0: loss 2.448332\n",
      "batch 1: loss 2.232373\n",
      "batch 2: loss 2.217708\n",
      "batch 3: loss 2.140573\n",
      "batch 4: loss 2.077614\n",
      "batch 5: loss 1.974723\n",
      "batch 6: loss 1.873117\n",
      "batch 7: loss 1.873246\n",
      "batch 8: loss 1.823415\n",
      "batch 9: loss 1.793207\n",
      "batch 10: loss 1.711310\n",
      "batch 11: loss 1.512110\n",
      "batch 12: loss 1.575150\n",
      "batch 13: loss 1.484524\n",
      "batch 14: loss 1.410708\n",
      "batch 15: loss 1.418751\n",
      "batch 16: loss 1.318666\n",
      "batch 17: loss 1.289241\n",
      "batch 18: loss 1.268923\n",
      "batch 19: loss 1.182100\n",
      "batch 20: loss 1.138317\n",
      "batch 21: loss 1.042774\n",
      "batch 22: loss 1.024451\n",
      "batch 23: loss 1.010646\n",
      "batch 24: loss 0.887569\n",
      "batch 25: loss 0.953281\n",
      "batch 26: loss 0.863570\n",
      "batch 27: loss 0.984607\n",
      "batch 28: loss 0.768714\n",
      "batch 29: loss 0.820731\n",
      "batch 30: loss 0.861115\n",
      "batch 31: loss 0.827887\n",
      "batch 32: loss 0.709560\n",
      "batch 33: loss 0.617104\n",
      "batch 34: loss 0.657366\n",
      "batch 35: loss 0.692014\n",
      "batch 36: loss 0.801132\n",
      "batch 37: loss 0.620606\n",
      "batch 38: loss 0.647980\n",
      "batch 39: loss 0.609436\n",
      "batch 40: loss 0.566316\n",
      "batch 41: loss 0.749932\n",
      "batch 42: loss 0.574099\n",
      "batch 43: loss 0.564018\n",
      "batch 44: loss 0.460701\n",
      "batch 45: loss 0.507876\n",
      "batch 46: loss 0.684219\n",
      "batch 47: loss 0.621897\n",
      "batch 48: loss 0.629285\n",
      "batch 49: loss 0.611502\n",
      "batch 50: loss 0.465818\n",
      "batch 51: loss 0.481400\n",
      "batch 52: loss 0.563956\n",
      "batch 53: loss 0.542892\n",
      "batch 54: loss 0.535961\n",
      "batch 55: loss 0.435453\n",
      "batch 56: loss 0.410261\n",
      "batch 57: loss 0.404571\n",
      "batch 58: loss 0.462261\n",
      "batch 59: loss 0.614786\n",
      "batch 60: loss 0.551208\n",
      "batch 61: loss 0.519954\n",
      "batch 62: loss 0.596415\n",
      "batch 63: loss 0.453549\n",
      "batch 64: loss 0.376144\n",
      "batch 65: loss 0.496248\n",
      "batch 66: loss 0.444162\n",
      "batch 67: loss 0.483764\n",
      "batch 68: loss 0.542343\n",
      "batch 69: loss 0.360391\n",
      "batch 70: loss 0.507260\n",
      "batch 71: loss 0.351589\n",
      "batch 72: loss 0.393895\n",
      "batch 73: loss 0.369951\n",
      "batch 74: loss 0.433514\n",
      "batch 75: loss 0.457457\n",
      "batch 76: loss 0.380582\n",
      "batch 77: loss 0.452893\n",
      "batch 78: loss 0.401799\n",
      "batch 79: loss 0.432842\n",
      "batch 80: loss 0.359233\n",
      "batch 81: loss 0.371874\n",
      "batch 82: loss 0.495340\n",
      "batch 83: loss 0.363183\n",
      "batch 84: loss 0.356969\n",
      "batch 85: loss 0.411612\n",
      "batch 86: loss 0.376976\n",
      "batch 87: loss 0.332653\n",
      "batch 88: loss 0.299243\n",
      "batch 89: loss 0.510692\n",
      "batch 90: loss 0.484285\n",
      "batch 91: loss 0.422445\n",
      "batch 92: loss 0.475129\n",
      "batch 93: loss 0.348064\n",
      "batch 94: loss 0.433989\n",
      "batch 95: loss 0.380818\n",
      "batch 96: loss 0.422484\n",
      "batch 97: loss 0.279050\n",
      "batch 98: loss 0.576077\n",
      "batch 99: loss 0.415044\n",
      "batch 100: loss 0.296163\n",
      "batch 101: loss 0.329242\n",
      "batch 102: loss 0.320258\n",
      "batch 103: loss 0.517404\n",
      "batch 104: loss 0.281534\n",
      "batch 105: loss 0.433197\n",
      "batch 106: loss 0.372395\n",
      "batch 107: loss 0.402198\n",
      "batch 108: loss 0.284276\n",
      "batch 109: loss 0.386908\n",
      "batch 110: loss 0.296125\n",
      "batch 111: loss 0.398987\n",
      "batch 112: loss 0.302885\n",
      "batch 113: loss 0.458071\n",
      "batch 114: loss 0.558708\n",
      "batch 115: loss 0.344182\n",
      "batch 116: loss 0.356407\n",
      "batch 117: loss 0.302237\n",
      "batch 118: loss 0.482792\n",
      "batch 119: loss 0.393536\n",
      "batch 120: loss 0.382086\n",
      "batch 121: loss 0.399994\n",
      "batch 122: loss 0.311164\n",
      "batch 123: loss 0.325476\n",
      "batch 124: loss 0.375147\n",
      "batch 125: loss 0.424886\n",
      "batch 126: loss 0.277093\n",
      "batch 127: loss 0.366920\n",
      "batch 128: loss 0.270564\n",
      "batch 129: loss 0.347372\n",
      "batch 130: loss 0.322371\n",
      "batch 131: loss 0.386865\n",
      "batch 132: loss 0.273632\n",
      "batch 133: loss 0.286744\n",
      "batch 134: loss 0.197165\n",
      "batch 135: loss 0.395511\n",
      "batch 136: loss 0.387142\n",
      "batch 137: loss 0.371773\n",
      "batch 138: loss 0.433765\n",
      "batch 139: loss 0.238869\n",
      "batch 140: loss 0.361353\n",
      "batch 141: loss 0.352249\n",
      "batch 142: loss 0.319088\n",
      "batch 143: loss 0.399054\n",
      "batch 144: loss 0.361849\n",
      "batch 145: loss 0.309469\n",
      "batch 146: loss 0.414043\n",
      "batch 147: loss 0.343740\n",
      "batch 148: loss 0.516683\n",
      "batch 149: loss 0.340201\n",
      "batch 150: loss 0.278933\n",
      "batch 151: loss 0.350159\n",
      "batch 152: loss 0.368243\n",
      "batch 153: loss 0.314314\n",
      "batch 154: loss 0.500829\n",
      "batch 155: loss 0.312584\n",
      "batch 156: loss 0.306547\n",
      "batch 157: loss 0.246714\n",
      "batch 158: loss 0.386887\n",
      "batch 159: loss 0.484069\n",
      "batch 160: loss 0.114321\n",
      "batch 161: loss 0.288287\n",
      "batch 162: loss 0.406639\n",
      "batch 163: loss 0.258065\n",
      "batch 164: loss 0.307977\n",
      "batch 165: loss 0.285997\n",
      "batch 166: loss 0.269864\n",
      "batch 167: loss 0.274704\n",
      "batch 168: loss 0.270080\n",
      "batch 169: loss 0.284438\n",
      "batch 170: loss 0.372681\n",
      "batch 171: loss 0.273194\n",
      "batch 172: loss 0.204457\n",
      "batch 173: loss 0.399013\n",
      "batch 174: loss 0.229818\n",
      "batch 175: loss 0.300536\n",
      "batch 176: loss 0.412859\n",
      "batch 177: loss 0.244211\n",
      "batch 178: loss 0.293025\n",
      "batch 179: loss 0.226813\n",
      "batch 180: loss 0.222595\n",
      "batch 181: loss 0.382103\n",
      "batch 182: loss 0.203140\n",
      "batch 183: loss 0.242499\n",
      "batch 184: loss 0.400165\n",
      "batch 185: loss 0.410524\n",
      "batch 186: loss 0.210254\n",
      "batch 187: loss 0.163271\n",
      "batch 188: loss 0.276875\n",
      "batch 189: loss 0.390199\n",
      "batch 190: loss 0.211724\n",
      "batch 191: loss 0.465993\n",
      "batch 192: loss 0.303804\n",
      "batch 193: loss 0.234850\n",
      "batch 194: loss 0.343185\n",
      "batch 195: loss 0.221387\n",
      "batch 196: loss 0.277932\n",
      "batch 197: loss 0.384009\n",
      "batch 198: loss 0.296641\n",
      "batch 199: loss 0.258287\n",
      "batch 200: loss 0.334797\n",
      "batch 201: loss 0.248627\n",
      "batch 202: loss 0.283398\n",
      "batch 203: loss 0.270521\n",
      "batch 204: loss 0.302548\n",
      "batch 205: loss 0.193717\n",
      "batch 206: loss 0.314465\n",
      "batch 207: loss 0.210308\n",
      "batch 208: loss 0.246759\n",
      "batch 209: loss 0.393518\n",
      "batch 210: loss 0.343584\n",
      "batch 211: loss 0.246687\n",
      "batch 212: loss 0.225475\n",
      "batch 213: loss 0.186217\n",
      "batch 214: loss 0.424464\n",
      "batch 215: loss 0.460406\n",
      "batch 216: loss 0.188635\n",
      "batch 217: loss 0.472745\n",
      "batch 218: loss 0.351137\n",
      "batch 219: loss 0.237196\n",
      "batch 220: loss 0.205719\n",
      "batch 221: loss 0.170493\n",
      "batch 222: loss 0.269798\n",
      "batch 223: loss 0.256246\n",
      "batch 224: loss 0.373415\n",
      "batch 225: loss 0.239236\n",
      "batch 226: loss 0.220824\n",
      "batch 227: loss 0.301280\n",
      "batch 228: loss 0.171346\n",
      "batch 229: loss 0.209258\n",
      "batch 230: loss 0.243727\n",
      "batch 231: loss 0.301397\n",
      "batch 232: loss 0.261750\n",
      "batch 233: loss 0.329269\n",
      "batch 234: loss 0.210462\n",
      "batch 235: loss 0.306580\n",
      "batch 236: loss 0.266964\n",
      "batch 237: loss 0.335612\n",
      "batch 238: loss 0.355083\n",
      "batch 239: loss 0.171203\n",
      "batch 240: loss 0.342197\n",
      "batch 241: loss 0.246513\n",
      "batch 242: loss 0.210587\n",
      "batch 243: loss 0.324462\n",
      "batch 244: loss 0.450200\n",
      "batch 245: loss 0.139543\n",
      "batch 246: loss 0.235047\n",
      "batch 247: loss 0.269004\n",
      "batch 248: loss 0.262310\n",
      "batch 249: loss 0.312387\n",
      "batch 250: loss 0.387605\n",
      "batch 251: loss 0.362208\n",
      "batch 252: loss 0.182862\n",
      "batch 253: loss 0.225273\n",
      "batch 254: loss 0.144876\n",
      "batch 255: loss 0.459628\n",
      "batch 256: loss 0.251311\n",
      "batch 257: loss 0.222396\n",
      "batch 258: loss 0.296852\n",
      "batch 259: loss 0.278570\n",
      "batch 260: loss 0.267825\n",
      "batch 261: loss 0.326777\n",
      "batch 262: loss 0.369407\n",
      "batch 263: loss 0.331147\n",
      "batch 264: loss 0.269566\n",
      "batch 265: loss 0.510351\n",
      "batch 266: loss 0.319623\n",
      "batch 267: loss 0.238252\n",
      "batch 268: loss 0.301790\n",
      "batch 269: loss 0.281889\n",
      "batch 270: loss 0.376450\n",
      "batch 271: loss 0.243934\n",
      "batch 272: loss 0.225122\n",
      "batch 273: loss 0.209152\n",
      "batch 274: loss 0.275151\n",
      "batch 275: loss 0.434863\n",
      "batch 276: loss 0.201001\n",
      "batch 277: loss 0.108073\n",
      "batch 278: loss 0.252330\n",
      "batch 279: loss 0.397336\n",
      "batch 280: loss 0.435161\n",
      "batch 281: loss 0.333138\n",
      "batch 282: loss 0.232401\n",
      "batch 283: loss 0.234797\n",
      "batch 284: loss 0.299713\n",
      "batch 285: loss 0.175675\n",
      "batch 286: loss 0.273617\n",
      "batch 287: loss 0.354667\n",
      "batch 288: loss 0.138773\n",
      "batch 289: loss 0.140191\n",
      "batch 290: loss 0.283471\n",
      "batch 291: loss 0.219373\n",
      "batch 292: loss 0.233633\n",
      "batch 293: loss 0.385970\n",
      "batch 294: loss 0.240910\n",
      "batch 295: loss 0.237337\n",
      "batch 296: loss 0.208095\n",
      "batch 297: loss 0.326349\n",
      "batch 298: loss 0.279197\n",
      "batch 299: loss 0.253864\n",
      "batch 300: loss 0.231125\n",
      "batch 301: loss 0.289364\n",
      "batch 302: loss 0.377299\n",
      "batch 303: loss 0.364925\n",
      "batch 304: loss 0.316242\n",
      "batch 305: loss 0.144773\n",
      "batch 306: loss 0.268720\n",
      "batch 307: loss 0.176904\n",
      "batch 308: loss 0.243475\n",
      "batch 309: loss 0.213883\n",
      "batch 310: loss 0.265286\n",
      "batch 311: loss 0.256179\n",
      "batch 312: loss 0.162641\n",
      "batch 313: loss 0.168122\n",
      "batch 314: loss 0.231574\n",
      "batch 315: loss 0.214950\n",
      "batch 316: loss 0.252829\n",
      "batch 317: loss 0.189746\n",
      "batch 318: loss 0.154875\n",
      "batch 319: loss 0.322564\n",
      "batch 320: loss 0.212979\n",
      "batch 321: loss 0.293382\n",
      "batch 322: loss 0.224854\n",
      "batch 323: loss 0.243553\n",
      "batch 324: loss 0.311878\n",
      "batch 325: loss 0.141444\n",
      "batch 326: loss 0.267803\n",
      "batch 327: loss 0.280126\n",
      "batch 328: loss 0.337854\n",
      "batch 329: loss 0.363298\n",
      "batch 330: loss 0.111082\n",
      "batch 331: loss 0.316539\n",
      "batch 332: loss 0.202848\n",
      "batch 333: loss 0.335473\n",
      "batch 334: loss 0.229835\n",
      "batch 335: loss 0.217373\n",
      "batch 336: loss 0.158589\n",
      "batch 337: loss 0.218446\n",
      "batch 338: loss 0.141230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 339: loss 0.299434\n",
      "batch 340: loss 0.183660\n",
      "batch 341: loss 0.170845\n",
      "batch 342: loss 0.215946\n",
      "batch 343: loss 0.286053\n",
      "batch 344: loss 0.257184\n",
      "batch 345: loss 0.244548\n",
      "batch 346: loss 0.307304\n",
      "batch 347: loss 0.178251\n",
      "batch 348: loss 0.190371\n",
      "batch 349: loss 0.186960\n",
      "batch 350: loss 0.212605\n",
      "batch 351: loss 0.202421\n",
      "batch 352: loss 0.169142\n",
      "batch 353: loss 0.237986\n",
      "batch 354: loss 0.130283\n",
      "batch 355: loss 0.259669\n",
      "batch 356: loss 0.154367\n",
      "batch 357: loss 0.248631\n",
      "batch 358: loss 0.182629\n",
      "batch 359: loss 0.361344\n",
      "batch 360: loss 0.210905\n",
      "batch 361: loss 0.265098\n",
      "batch 362: loss 0.272234\n",
      "batch 363: loss 0.197086\n",
      "batch 364: loss 0.117507\n",
      "batch 365: loss 0.190136\n",
      "batch 366: loss 0.354583\n",
      "batch 367: loss 0.244001\n",
      "batch 368: loss 0.226531\n",
      "batch 369: loss 0.281699\n",
      "batch 370: loss 0.311118\n",
      "batch 371: loss 0.336741\n",
      "batch 372: loss 0.199976\n",
      "batch 373: loss 0.288680\n",
      "batch 374: loss 0.166038\n",
      "batch 375: loss 0.190006\n",
      "batch 376: loss 0.175793\n",
      "batch 377: loss 0.139932\n",
      "batch 378: loss 0.291172\n",
      "batch 379: loss 0.220862\n",
      "batch 380: loss 0.269636\n",
      "batch 381: loss 0.257162\n",
      "batch 382: loss 0.103927\n",
      "batch 383: loss 0.448421\n",
      "batch 384: loss 0.284107\n",
      "batch 385: loss 0.132320\n",
      "batch 386: loss 0.321592\n",
      "batch 387: loss 0.308561\n",
      "batch 388: loss 0.273613\n",
      "batch 389: loss 0.191091\n",
      "batch 390: loss 0.287512\n",
      "batch 391: loss 0.171205\n",
      "batch 392: loss 0.318323\n",
      "batch 393: loss 0.264298\n",
      "batch 394: loss 0.179841\n",
      "batch 395: loss 0.206226\n",
      "batch 396: loss 0.240916\n",
      "batch 397: loss 0.410301\n",
      "batch 398: loss 0.204709\n",
      "batch 399: loss 0.213776\n",
      "batch 400: loss 0.278096\n",
      "batch 401: loss 0.334927\n",
      "batch 402: loss 0.160388\n",
      "batch 403: loss 0.269288\n",
      "batch 404: loss 0.366212\n",
      "batch 405: loss 0.257277\n",
      "batch 406: loss 0.212995\n",
      "batch 407: loss 0.265815\n",
      "batch 408: loss 0.302624\n",
      "batch 409: loss 0.207567\n",
      "batch 410: loss 0.248466\n",
      "batch 411: loss 0.165131\n",
      "batch 412: loss 0.269398\n",
      "batch 413: loss 0.195893\n",
      "batch 414: loss 0.185039\n",
      "batch 415: loss 0.222582\n",
      "batch 416: loss 0.188519\n",
      "batch 417: loss 0.230258\n",
      "batch 418: loss 0.297612\n",
      "batch 419: loss 0.323066\n",
      "batch 420: loss 0.207458\n",
      "batch 421: loss 0.306785\n",
      "batch 422: loss 0.233736\n",
      "batch 423: loss 0.297306\n",
      "batch 424: loss 0.284292\n",
      "batch 425: loss 0.248264\n",
      "batch 426: loss 0.169225\n",
      "batch 427: loss 0.156034\n",
      "batch 428: loss 0.242209\n",
      "batch 429: loss 0.198817\n",
      "batch 430: loss 0.187900\n",
      "batch 431: loss 0.196438\n",
      "batch 432: loss 0.183188\n",
      "batch 433: loss 0.146880\n",
      "batch 434: loss 0.280253\n",
      "batch 435: loss 0.189724\n",
      "batch 436: loss 0.250136\n",
      "batch 437: loss 0.175564\n",
      "batch 438: loss 0.167020\n",
      "batch 439: loss 0.395922\n",
      "batch 440: loss 0.262388\n",
      "batch 441: loss 0.278013\n",
      "batch 442: loss 0.360995\n",
      "batch 443: loss 0.344401\n",
      "batch 444: loss 0.224272\n",
      "batch 445: loss 0.243109\n",
      "batch 446: loss 0.251386\n",
      "batch 447: loss 0.381522\n",
      "batch 448: loss 0.157584\n",
      "batch 449: loss 0.226409\n",
      "batch 450: loss 0.215926\n",
      "batch 451: loss 0.248673\n",
      "batch 452: loss 0.360534\n",
      "batch 453: loss 0.151381\n",
      "batch 454: loss 0.195285\n",
      "batch 455: loss 0.111333\n",
      "batch 456: loss 0.231281\n",
      "batch 457: loss 0.288753\n",
      "batch 458: loss 0.297234\n",
      "batch 459: loss 0.182424\n",
      "batch 460: loss 0.266045\n",
      "batch 461: loss 0.197800\n",
      "batch 462: loss 0.130790\n",
      "batch 463: loss 0.285426\n",
      "batch 464: loss 0.200007\n",
      "batch 465: loss 0.282725\n",
      "batch 466: loss 0.235767\n",
      "batch 467: loss 0.217134\n",
      "batch 468: loss 0.317963\n",
      "batch 469: loss 0.250264\n",
      "batch 470: loss 0.220324\n",
      "batch 471: loss 0.329054\n",
      "batch 472: loss 0.190739\n",
      "batch 473: loss 0.215190\n",
      "batch 474: loss 0.174838\n",
      "batch 475: loss 0.144261\n",
      "batch 476: loss 0.329054\n",
      "batch 477: loss 0.260563\n",
      "batch 478: loss 0.115247\n",
      "batch 479: loss 0.226434\n",
      "batch 480: loss 0.266627\n",
      "batch 481: loss 0.185652\n",
      "batch 482: loss 0.238715\n",
      "batch 483: loss 0.224374\n",
      "batch 484: loss 0.230955\n",
      "batch 485: loss 0.196923\n",
      "batch 486: loss 0.185104\n",
      "batch 487: loss 0.075193\n",
      "batch 488: loss 0.295864\n",
      "batch 489: loss 0.229785\n",
      "batch 490: loss 0.167127\n",
      "batch 491: loss 0.246678\n",
      "batch 492: loss 0.230640\n",
      "batch 493: loss 0.274741\n",
      "batch 494: loss 0.141553\n",
      "batch 495: loss 0.267079\n",
      "batch 496: loss 0.211876\n",
      "batch 497: loss 0.182684\n",
      "batch 498: loss 0.217650\n",
      "batch 499: loss 0.178876\n",
      "batch 500: loss 0.191141\n",
      "batch 501: loss 0.302196\n",
      "batch 502: loss 0.194585\n",
      "batch 503: loss 0.189038\n",
      "batch 504: loss 0.150333\n",
      "batch 505: loss 0.165959\n",
      "batch 506: loss 0.162148\n",
      "batch 507: loss 0.088459\n",
      "batch 508: loss 0.192507\n",
      "batch 509: loss 0.115973\n",
      "batch 510: loss 0.152814\n",
      "batch 511: loss 0.152203\n",
      "batch 512: loss 0.234744\n",
      "batch 513: loss 0.310605\n",
      "batch 514: loss 0.165384\n",
      "batch 515: loss 0.155514\n",
      "batch 516: loss 0.160278\n",
      "batch 517: loss 0.176192\n",
      "batch 518: loss 0.142125\n",
      "batch 519: loss 0.184037\n",
      "batch 520: loss 0.191294\n",
      "batch 521: loss 0.238140\n",
      "batch 522: loss 0.282767\n",
      "batch 523: loss 0.153771\n",
      "batch 524: loss 0.124890\n",
      "batch 525: loss 0.175655\n",
      "batch 526: loss 0.160126\n",
      "batch 527: loss 0.189339\n",
      "batch 528: loss 0.191685\n",
      "batch 529: loss 0.193339\n",
      "batch 530: loss 0.107527\n",
      "batch 531: loss 0.159509\n",
      "batch 532: loss 0.384546\n",
      "batch 533: loss 0.229602\n",
      "batch 534: loss 0.279364\n",
      "batch 535: loss 0.255253\n",
      "batch 536: loss 0.200126\n",
      "batch 537: loss 0.260974\n",
      "batch 538: loss 0.253480\n",
      "batch 539: loss 0.169716\n",
      "batch 540: loss 0.444837\n",
      "batch 541: loss 0.168427\n",
      "batch 542: loss 0.139837\n",
      "batch 543: loss 0.104871\n",
      "batch 544: loss 0.207312\n",
      "batch 545: loss 0.286861\n",
      "batch 546: loss 0.247580\n",
      "batch 547: loss 0.131658\n",
      "batch 548: loss 0.205811\n",
      "batch 549: loss 0.161259\n",
      "batch 550: loss 0.279347\n",
      "batch 551: loss 0.185850\n",
      "batch 552: loss 0.079373\n",
      "batch 553: loss 0.256639\n",
      "batch 554: loss 0.180696\n",
      "batch 555: loss 0.098509\n",
      "batch 556: loss 0.183393\n",
      "batch 557: loss 0.158530\n",
      "batch 558: loss 0.163315\n",
      "batch 559: loss 0.153502\n",
      "batch 560: loss 0.258882\n",
      "batch 561: loss 0.192491\n",
      "batch 562: loss 0.210682\n",
      "batch 563: loss 0.187737\n",
      "batch 564: loss 0.251148\n",
      "batch 565: loss 0.342518\n",
      "batch 566: loss 0.216052\n",
      "batch 567: loss 0.185021\n",
      "batch 568: loss 0.165692\n",
      "batch 569: loss 0.098190\n",
      "batch 570: loss 0.182160\n",
      "batch 571: loss 0.204697\n",
      "batch 572: loss 0.201599\n",
      "batch 573: loss 0.280711\n",
      "batch 574: loss 0.094331\n",
      "batch 575: loss 0.196285\n",
      "batch 576: loss 0.174220\n",
      "batch 577: loss 0.254319\n",
      "batch 578: loss 0.211000\n",
      "batch 579: loss 0.231049\n",
      "batch 580: loss 0.288491\n",
      "batch 581: loss 0.225056\n",
      "batch 582: loss 0.160796\n",
      "batch 583: loss 0.186806\n",
      "batch 584: loss 0.261275\n",
      "batch 585: loss 0.166617\n",
      "batch 586: loss 0.272933\n",
      "batch 587: loss 0.149717\n",
      "batch 588: loss 0.111630\n",
      "batch 589: loss 0.122294\n",
      "batch 590: loss 0.143626\n",
      "batch 591: loss 0.099990\n",
      "batch 592: loss 0.239469\n",
      "batch 593: loss 0.186636\n",
      "batch 594: loss 0.187511\n",
      "batch 595: loss 0.182154\n",
      "batch 596: loss 0.168813\n",
      "batch 597: loss 0.218164\n",
      "batch 598: loss 0.181886\n",
      "batch 599: loss 0.140896\n"
     ]
    }
   ],
   "source": [
    "num_batches = int(data_loader.num_train_data // batch_size * num_epochs)\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.943500\n"
     ]
    }
   ],
   "source": [
    "sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "num_batches = int(data_loader.num_test_data // batch_size)\n",
    "for batch_index in range(num_batches):\n",
    "    start_index, end_index = batch_index * batch_size, (batch_index + 1) * batch_size\n",
    "    y_pred = model.predict(data_loader.test_data[start_index: end_index])\n",
    "    sparse_categorical_accuracy.update_state(y_true=data_loader.test_label[start_index: end_index], y_pred=y_pred)\n",
    "print(\"test accuracy: %f\" % sparse_categorical_accuracy.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
